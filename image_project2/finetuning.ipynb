{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tifffile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from transformers import (\n",
    "    SegformerForSemanticSegmentation, \n",
    "    TrainingArguments, Trainer, \n",
    "    SegformerImageProcessor)\n",
    "from datasets import Dataset, Image\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained models\n",
    "# MODEL_CHECKPOINT = 'nvidia/mit-b0'\n",
    "# MODEL_CHECKPOINT = 'nvidia/mit-b1'\n",
    "# MODEL_CHECKPOINT = 'nvidia/mit-b2'\n",
    "# MODEL_CHECKPOINT = 'nvidia/mit-b3'\n",
    "MODEL_CHECKPOINT = 'nvidia/mit-b4'\n",
    "# MODEL_CHECKPOINT = 'nvidia/mit-b5'\n",
    "# MODEL_CHECKPOINT = 'nvidia/segformer-b4-finetuned-cityscapes-1024-1024'\n",
    "\n",
    "VAL_SIZE = 0.1\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LR = 0.00006\n",
    "\n",
    "IMG_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chldy\\anaconda3\\envs\\torch_seg\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chldy\\.cache\\huggingface\\hub\\models--nvidia--mit-b4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b4 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'background', 1: 'water'}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        #self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = self.create_transforms()\n",
    "        \n",
    "        #sub_path = \"training\" if self.train else \"validation\"\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # read images\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "          image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        \n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "          annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
    "    \n",
    "    def create_transforms(self):\n",
    "        _transforms = A.Compose([\n",
    "            A.Resize(height=512, width=512), \n",
    "            ToTensorV2(),\n",
    "        ], p = 1,\n",
    "        )\n",
    "        return _transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = np.array(Image.open(os.path.join(self.img_dir, self.images[idx])).convert('RGB'))\n",
    "        mask  = np.array(Image.open(os.path.join(self.ann_dir, self.annotations[idx])).convert('L'))\n",
    "        if image.shape[-1] == 4:\n",
    "            image = image[:, :, :4]\n",
    "        encoded_inputs =  self.transforms(image = image, mask = mask)\n",
    "\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir,annotations_dir,  transform=None):\n",
    "        \"\"\"\n",
    "        annotation_dir (string): 메타데이터가 있는 JSON 파일의 경로\n",
    "        img_dir (string): 모든 이미지가 있는 디렉토리의 경로\n",
    "        transform (callable, optional): 샘플에 적용될 선택적 변환\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.annotation_dir= annotations_dir\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        return len(label_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, label_list[idx].split('.')[0]+'.'+label_list[idx].split('.')[1])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except (IOError, OSError) as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # faceExp_uploader 부분만 라벨로 사용\n",
    "        with open(self.annotation_dir+'/'+label_list[idx],'r', encoding='utf-8') as f:\n",
    "            self.image_labels=json.load(f)\n",
    "        label = self.image_labels['faceExp_uploader']\n",
    "        label_to_int = {'기쁨': 0, '당황': 1, '중립': 2}\n",
    "\n",
    "        # 문자열 라벨을 정수로 매핑\n",
    "        label_int = label_to_int[label]\n",
    "        label_tensor = torch.tensor(label_int, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        else:\n",
    "            # 기본적으로 이미지를 Tensor로 변환\n",
    "            transform = ToTensor()\n",
    "            image_tensor = transform(image)\n",
    "        \n",
    "        return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor\n",
    "\n",
    "root_dir = '/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img'\n",
    "\n",
    "\n",
    "train_dataset = SemanticSegmentationDataset(root_dir=root_dir)\n",
    "valid_dataset = SemanticSegmentationDataset(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU mehtod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):  \n",
    "   print(\"Epoch:\", epoch)\n",
    "   for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"image\"].to(device).float()\n",
    "        labels = batch[\"mask\"].to(device).long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate\n",
    "        with torch.no_grad():\n",
    "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "          predicted = upsampled_logits.argmax(dim=1)\n",
    "          \n",
    "          # note that the metric expects predictions + labels as numpy arrays\n",
    "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "\n",
    "        # let's print loss and metrics every 100 batches\n",
    "        if idx % 100 == 0:\n",
    "          metrics = metric._compute(references = labels.cpu(), \n",
    "                                    predictions = predicted.cpu(),\n",
    "                                    num_labels=len(id2label), \n",
    "                                   ignore_index=255,\n",
    "                                   reduce_labels=False, # we've already reduced the labels before)\n",
    "          )\n",
    "\n",
    "          print(\"Loss:\", loss.item())\n",
    "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "          print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
