{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 ID 및 레이블 매핑 정의\n",
    "\n",
    "id2label = {\n",
    "            0: 'background',\n",
    "            1: 'common_road',\n",
    "            2: 'common_tree',\n",
    "            3: 'field_corps',\n",
    "            4: 'field_furrow',\n",
    "            5: 'field_levee',\n",
    "            6: 'orchard_road',\n",
    "            7: 'orchard_tree',\n",
    "            8: 'paddy_after_driving',\n",
    "            9: 'paddy_before_driving',\n",
    "            10: 'paddy_edge',\n",
    "            11: 'paddy_rice',\n",
    "            12: 'paddy_water' \n",
    "        }\n",
    "\n",
    "\n",
    "# 반대 매핑 생성\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from skimage.draw import polygon2mask\n",
    "import cv2\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.label2id = {v: k for k, v in id2label.items()}  # 레이블을 ID로 매핑\n",
    "\n",
    "        self.label_list = os.listdir(self.ann_dir)\n",
    "        self.img_paths = []\n",
    "        self.ann_paths = []\n",
    "        \n",
    "        # 미리 경로 계산\n",
    "        for label in self.label_list:\n",
    "            with open(os.path.join(self.ann_dir, label), 'r', encoding='utf-8') as f:\n",
    "                img_info = json.load(f)\n",
    "            img_path = os.path.join(self.img_dir, img_info['name'])\n",
    "            self.img_paths.append(img_path)\n",
    "            self.ann_paths.append(os.path.join(self.ann_dir, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        ann_path = self.ann_paths[idx]\n",
    "        \n",
    "        with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "            img_info = json.load(f)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = np.full((image.height, image.width), 0, dtype=np.int32)  # 초기값을 0으로 설정\n",
    "        \n",
    "        for obj in img_info['objects']:\n",
    "            class_id = self.get_class_index(obj['label'])\n",
    "            for pos in obj['position']:\n",
    "                coords = [(y, x) for x, y in zip(pos[::2], pos[1::2])] #pos[::2]는 짝수 인덱스(모든 x 좌표), pos[1::2]는 홀수 인덱스(모든 y 좌표)\n",
    "                # print(f\"Coords for {obj['label']} with class ID {class_id}: {coords}\")  # 디버깅용 좌표 출력\n",
    "\n",
    "                 # 좌표가 이미지 경계를 벗어나는지 확인\n",
    "                out_of_bounds_coords = [(x, y) for y, x in coords if x < 0 or x >= image.width or y < 0 or y >= image.height]\n",
    "                if out_of_bounds_coords:\n",
    "                    #print(f\"Warning: Some coordinates for {obj['label']} are out of image bounds: {out_of_bounds_coords}\")\n",
    "                    coords = [(max(0, min(image.height - 0.1, y)), max(0, min(image.width - 0.1, x))) for y, x in coords] # 이미지 벗어나는 좌표 클리핑\n",
    "\n",
    "            \n",
    "\n",
    "                poly_mask = polygon2mask((image.height, image.width), coords)\n",
    "\n",
    "                # 디버깅용으로 poly_mask가 True인 위치 출력\n",
    "                # true_indices = np.where(poly_mask)\n",
    "                # true_positions = list(zip(true_indices[0], true_indices[1]))\n",
    "                # print(f\"True positions in poly_mask: {true_positions[:10]}\")  # 처음 10개의 위치만 출력\n",
    "\n",
    "                mask[poly_mask] = class_id\n",
    "\n",
    "                # # 개별 마스크 시각화(디버깅)\n",
    "                # plt.figure(figsize=(6, 6))\n",
    "                # plt.imshow(mask, cmap='gray')\n",
    "                # plt.title(f\"{obj['label']} with class ID {class_id}\")\n",
    "                # plt.show()\n",
    "                \n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=np.array(image), mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "        # if self.transforms:\n",
    "        #     image = self.transforms(image)\n",
    "        \n",
    "        # mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        # return image, mask\n",
    "\n",
    "    def get_class_index(self, label):\n",
    "        return self.label2id.get(label, 0)  # 기본값 0 (배경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\torch_seg\\lib\\site-packages\\pydantic\\main.py:176: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 \n",
    "\n",
    "# ### Data augmentation을 고려할 필요가 있음\n",
    "\n",
    "# train_transforms = T.Compose([\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# val_transforms = T.Compose([\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ColorJitter(p=0.5),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.1),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "    ], p=1.0),\n",
    "    A.OneOf([\n",
    "        A.RandomRain(p=0.2),\n",
    "        A.RandomFog(p=0.2),\n",
    "        A.RandomShadow(p=0.2),\n",
    "    ], p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = CustomDataset(\"C:/Users/USER/Desktop/resized_train_images\",\"C:/Users/USER/Desktop/resized_train_annotations\", transforms=train_transforms)\n",
    "val_dataset = CustomDataset(\"C:/Users/USER/Desktop/resized_valid_images\", \"C:/Users/USER/Desktop/resized_valid_annotations\", transforms=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=13, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=13, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchldyddn98\u001b[0m (\u001b[33mchoiyw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\USER\\Desktop\\yong\\deep_learning_bootcamp\\image_project2\\wandb\\run-20240618_235447-b3_b2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/choiyw/knowledge_distillation/runs/b3_b2' target=\"_blank\">b3_b2</a></strong> to <a href='https://wandb.ai/choiyw/knowledge_distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/choiyw/knowledge_distillation' target=\"_blank\">https://wandb.ai/choiyw/knowledge_distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/choiyw/knowledge_distillation/runs/b3_b2' target=\"_blank\">https://wandb.ai/choiyw/knowledge_distillation/runs/b3_b2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/choiyw/knowledge_distillation/runs/b3_b2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x24fa78252a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start_epoch = 6  # 시작 에폭 설정\n",
    "# num_epochs = 10  # 총 학습 에폭 설정\n",
    "import wandb\n",
    "wandb.init(project=\"knowledge_distillation\", id='b3_b2')\n",
    "\n",
    "# # wandb 초기화\n",
    "# wandb.init(project=\"uncategorized_project\", resume=\"allow\", id=\"lemon-cherry-45\")\n",
    "# wandb.config.update({\n",
    "#     \"epochs\": num_epochs,\n",
    "#     \"batch_size\": 16,\n",
    "#     \"start_epoch\": start_epoch,\n",
    "#     \"learning_rate\": 0.00006,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\torch_seg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.3.1 available.\n",
      "Epoch 1/10:   8%|▊         | 69/831 [01:35<17:53,  1.41s/batch, batch_loss=161092.6094]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import evaluate\n",
    "import os\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# 저장할 폴더 지정 및 생성\n",
    "model_save_path = \"saved_models\"\n",
    "model_filename = \"best_model.pth\"\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "full_model_path = os.path.join(model_save_path, model_filename)\n",
    "\n",
    "# 설정 변수\n",
    "gradient_accumulation_steps = 4\n",
    "best_mean_iou = 0.0\n",
    "\n",
    "# Mean IoU 계산 함수 정의\n",
    "def compute_mean_iou(predictions, references, num_labels, ignore_index):\n",
    "    classes_in_labels = torch.unique(references)\n",
    "    if ignore_index is not None:\n",
    "        classes_in_labels = classes_in_labels[classes_in_labels != ignore_index]\n",
    "    \n",
    "    iou_list = []\n",
    "    for cls in range(num_labels):\n",
    "        if cls in classes_in_labels:\n",
    "            intersection = torch.sum((predictions == cls) & (references == cls)).item()\n",
    "            union = torch.sum((predictions == cls) | (references == cls)).item()\n",
    "            iou = intersection / union if union != 0 else 0\n",
    "            iou_list.append(iou)\n",
    "    \n",
    "    mean_iou = sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "    return mean_iou\n",
    "\n",
    "# 검증 함수\n",
    "def validate_amp(model, val_loader, device, num_labels=13, ignore_index=255):\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_pixels = 0\n",
    "    iou_metric = evaluate.load(\"mean_iou\", trust_remote_code=True)  # 각 검증 루프마다 메트릭을 새로 초기화\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        pbar = tqdm(total=len(val_loader), desc=\"Validating\", unit=\"batch\")  # tqdm을 사용하여 검증 진행 상황 표시\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch[0].to(device).float()  # 입력 데이터를 장치로 이동\n",
    "            labels = batch[1].to(device).long()  # 라벨 데이터를 장치로 이동\n",
    "            \n",
    "            with autocast():  # 자동 혼합 정밀도(AMP)를 사용하여 연산\n",
    "                outputs = model(pixel_values=pixel_values)  \n",
    "                logits = outputs.logits  # 로짓 값\n",
    "                upsampled_logits = F.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)  # 로짓을 라벨 크기로 보간\n",
    "                loss = F.cross_entropy(upsampled_logits, labels, ignore_index=ignore_index)  # 교차 엔트로피 손실 계산\n",
    "                \n",
    "            total_loss += loss.item()  # 총 손실 계산\n",
    "            predicted = upsampled_logits.argmax(dim=1)  # 예측값 계산\n",
    "\n",
    "            # Pixel accuracy 계산\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total = (labels != ignore_index).sum().item()\n",
    "            total_correct += correct\n",
    "            total_pixels += total\n",
    "\n",
    "            iou_metric.add_batch(predictions=predicted, references=labels)  # 메트릭에 배치 추가\n",
    "            pbar.update(1) \n",
    "\n",
    "        pbar.close()  \n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)  # 평균 손실 계산\n",
    "    mean_iou = compute_mean_iou(predicted, labels, num_labels, ignore_index)  # Mean IoU 계산\n",
    "    pixel_accuracy = total_correct / total_pixels  # Pixel Accuracy 계산\n",
    "    return avg_loss, mean_iou, pixel_accuracy  # 평균 손실과 Mean IoU 반환\n",
    "\n",
    "# Knowledge Distillation 손실 함수 정의\n",
    "def distillation_loss(student_outputs, teacher_outputs, labels, T=2.0, alpha=0.5):\n",
    "    soft_teacher_outputs = F.softmax(teacher_outputs / T, dim=1)\n",
    "    soft_student_outputs = F.log_softmax(student_outputs / T, dim=1)\n",
    "    loss_soft = F.kl_div(soft_student_outputs, soft_teacher_outputs, reduction='batchmean') * (T * T)\n",
    "    loss_hard = F.cross_entropy(student_outputs, labels)\n",
    "    return loss_soft * alpha + loss_hard * (1.0 - alpha)\n",
    "\n",
    "# 학습 및 검증 함수 정의\n",
    "def train_student_model(teacher_model, student_model, train_loader, val_loader, optimizer, num_epochs=10):\n",
    "    best_mean_iou = 0.0\n",
    "    gradient_accumulation_steps = 4\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            pixel_values = batch[0].to(device).float()\n",
    "            labels = batch[1].to(device).long()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(pixel_values=pixel_values).logits\n",
    "                    teacher_outputs_upsampled = F.interpolate(teacher_outputs, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "                student_outputs = student_model(pixel_values=pixel_values).logits\n",
    "\n",
    "                # 학생 모델의 출력을 라벨 크기로 맞추기\n",
    "                student_outputs_upsampled = F.interpolate(student_outputs, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "                loss = distillation_loss(student_outputs_upsampled, teacher_outputs_upsampled, labels) / gradient_accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (idx + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item() * gradient_accumulation_steps\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(batch_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        val_loss, mean_iou, pixel_accuracy = validate_amp(student_model, val_loader, device)\n",
    "        wandb.log({\"Validation Loss\": val_loss, \"Mean IOU\": mean_iou, \"Pixel Accuracy\": pixel_accuracy})\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Mean IOU: {mean_iou}, Pixel Accuracy: {pixel_accuracy}\")\n",
    "\n",
    "        # 모델 저장\n",
    "        if mean_iou > best_mean_iou:\n",
    "            best_mean_iou = mean_iou\n",
    "            torch.save(student_model.state_dict(), full_model_path)\n",
    "            print(f\"Saved best model to {full_model_path}\")\n",
    "\n",
    "# 모델 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model_name = \"nvidia/segformer-b3-finetuned-cityscapes-1024-1024\"\n",
    "student_model_name = \"nvidia/segformer-b2-finetuned-cityscapes-1024-1024\"  # 경량화된 모델\n",
    "\n",
    "teacher_model = SegformerForSemanticSegmentation.from_pretrained(teacher_model_name)\n",
    "teacher_model.config.num_labels = 13\n",
    "teacher_model.decode_head.classifier = torch.nn.Conv2d(768, teacher_model.config.num_labels, kernel_size=1)\n",
    "teacher_model.load_state_dict(torch.load(\"C:/Users/USER/Desktop/yong/deep_learning_bootcamp/image_project2/saved_models/b3_10epoch_512288.pth\", map_location=device))\n",
    "teacher_model.eval()\n",
    "teacher_model.to(device)\n",
    "\n",
    "student_model = SegformerForSemanticSegmentation.from_pretrained(student_model_name)\n",
    "student_model.config.num_labels = 13\n",
    "student_model.decode_head.classifier = torch.nn.Conv2d(768, student_model.config.num_labels, kernel_size=1)\n",
    "student_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 학습 및 검증\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=0.00006)\n",
    "train_student_model(teacher_model, student_model, train_loader, val_loader, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
