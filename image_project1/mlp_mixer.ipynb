{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model = timm.create_model('mixer_b16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "checkpoint = torch.load(\"C:/Users/USER/Downloads/mlp_mixer_img21_b16.pth\", map_location='cpu') # GPU 환경이 아닌 경우 'cpu'를 사용합니다.\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 모델의 마지막 분류기 부분을 새로운 클래스 수에 맞게 변경\n",
    "num_classes = 3  # 새로운 클래스 수\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력층만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 모든 파라미터를 동결\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 출력층 파라미터만 학습 가능하도록 설정\n",
    "model.head.weight.requires_grad = True\n",
    "model.head.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (stem): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir,annotations_dir,  transform=None):\n",
    "        \"\"\"\n",
    "        annotation_dir (string): 메타데이터가 있는 JSON 파일의 경로\n",
    "        img_dir (string): 모든 이미지가 있는 디렉토리의 경로\n",
    "        transform (callable, optional): 샘플에 적용될 선택적 변환\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.annotation_dir= annotations_dir\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        return len(label_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, label_list[idx].split('.')[0]+'.'+label_list[idx].split('.')[1])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except (IOError, OSError) as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # faceExp_uploader 부분만 라벨로 사용\n",
    "        with open(self.annotation_dir+'/'+label_list[idx],'r', encoding='utf-8') as f:\n",
    "            self.image_labels=json.load(f)\n",
    "        label = self.image_labels['faceExp_uploader']\n",
    "        label_to_int = {'기쁨': 0, '당황': 1, '중립': 2}\n",
    "\n",
    "        # 문자열 라벨을 정수로 매핑\n",
    "        label_int = label_to_int[label]\n",
    "        label_tensor = torch.tensor(label_int, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        else:\n",
    "            # 기본적으로 이미지를 Tensor로 변환\n",
    "            transform = ToTensor()\n",
    "            image_tensor = transform(image)\n",
    "        \n",
    "        return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모폴로지 데이터 전처리\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "class MorphologyTransform:\n",
    "    def __call__(self, img):\n",
    "        # img는 PIL 이미지\n",
    "        img = np.array(img)\n",
    "        kernel = np.ones((5,5), np.uint8)\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "        return Image.fromarray(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge 감지\n",
    "class EdgeDetectionTransform:\n",
    "    def __call__(self, img):\n",
    "        # img는 PIL 이미지\n",
    "        img = np.array(img)\n",
    "        img = cv2.Canny(img, 100, 200)\n",
    "        return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(img_size, train_img_dir,train_annotation_dir, test_img_dir, test_annotation_dir, train_batch_size, eval_batch_size):\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        #주어진 이미지에서 임의의 크기 및 비율로 크롭한 후, 지정된 크기로 이미지를 리사이즈합니다, 5%~100% 사이의 영역을 crop 하여 resizing, crop되는 부분은 랜덤, ex) 중앙하단, 왼쪽 상단 등\n",
    "        transforms.RandomResizedCrop((img_size, img_size), scale=(0.05, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # 픽셀에 normalize 적용, 각 채널의 평균과 표준 편차, pre trained된 데이터의 통계를 기반으로 설정\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    trainset = CustomDataset(img_dir=train_img_dir,\n",
    "                             annotations_dir=train_annotation_dir,\n",
    "                             transform=transform_train)\n",
    "    testset = CustomDataset(img_dir=test_img_dir,\n",
    "                            annotations_dir=test_annotation_dir,\n",
    "                            transform=transform_test)\n",
    "    \n",
    "\n",
    "    train_sampler = RandomSampler(trainset) # 데이터셋에서 무작위로 샘플을 선택,  데이터셋의 인덱스를 무작위로 섞어서 데이터의 순서를 랜덤하게 배치, 모델이 순서에 의존하지 않고 학습함\n",
    "    test_sampler = SequentialSampler(testset) # 데이터셋에서 순차적으로 샘플을 선택, 데이터를 처음부터 끝까지 순서대로 샘플링\n",
    "    train_loader = DataLoader(trainset,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=train_batch_size,\n",
    "                              num_workers=0,\n",
    "                              pin_memory=True)\n",
    "    test_loader = DataLoader(testset,\n",
    "                             sampler=test_sampler,\n",
    "                             batch_size=eval_batch_size,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True) if testset is not None else None\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 시 하이퍼 파라미터 지정\n",
    "num_epochs=15\n",
    "train_batch_size = 64  # 훈련 배치 크기\n",
    "eval_batch_size = 64  # 평가 배치 크기\n",
    "train_img_dir= 'C:/Users/USER/Desktop/data/train_img'\n",
    "train_annotation_dir= 'C:/Users/USER/Desktop/data/train_label'\n",
    "test_img_dir='C:/Users/USER/Desktop/data/valid_img'\n",
    "test_annotaion_dir='C:/Users/USER/Desktop/data/valid_label'\n",
    "name= 'mlp_mixer_class_3_final'\n",
    "output_dir= 'output'\n",
    "eval_every = 113  # 몇 스텝마다 평가를 할 것인지\n",
    "learning_rate = 3e-2  # 초기 학습률\n",
    "weight_decay = 0  # 가중치 감소율\n",
    "sweight_decay = 0  # 가중치 감소율\n",
    "num_steps = eval_every * num_epochs  # 총 훈련 스텝\n",
    "seed = 42  # 초기화를 위한 랜덤 시드\n",
    "n_gpu=1\n",
    "gradient_accumulation_steps = 1  # 업데이트를 위해 누적할 스텝 수\n",
    "warmup_steps = 500  # 웜업을 위한 스텝 수\n",
    "max_grad_norm = 1.0  # 최대 그래디언트 노름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\n"
     ]
    }
   ],
   "source": [
    "cd Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=1.02996): 100%|| 29/29 [03:04<00:00,  6.37s/it]00:07,  7.58s/it]\n",
      "Epoch 1/15 (Step 113 / 1695) (Loss=0.85477): 100%|| 113/113 [16:23<00:00, 61.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 113\n",
      "Valid Loss: 0.92807\n",
      "Valid Accuracy: 0.53500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 (Step 113 / 1695) (Loss=0.85477): 100%|| 113/113 [16:23<00:00,  8.71s/it]\n",
      "Validating... (loss=1.19834): 100%|| 29/29 [02:52<00:00,  5.93s/it]00:07,  7.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 226\n",
      "Valid Loss: 0.66047\n",
      "Valid Accuracy: 0.72278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 (Step 226 / 1695) (Loss=0.60045): 100%|| 113/113 [16:01<00:00,  8.51s/it]\n",
      "Validating... (loss=1.05755): 100%|| 29/29 [02:50<00:00,  5.86s/it]00:07,  7.17s/it]\n",
      "Epoch 3/15 (Step 339 / 1695) (Loss=0.45560): 100%|| 113/113 [15:55<00:00, 57.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 339\n",
      "Valid Loss: 0.61099\n",
      "Valid Accuracy: 0.76000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 (Step 339 / 1695) (Loss=0.45560): 100%|| 113/113 [15:55<00:00,  8.45s/it]\n",
      "Validating... (loss=0.75156): 100%|| 29/29 [02:48<00:00,  5.80s/it]00:06,  6.77s/it]\n",
      "Epoch 4/15 (Step 452 / 1695) (Loss=0.53893): 100%|| 113/113 [15:54<00:00, 56.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 452\n",
      "Valid Loss: 0.56051\n",
      "Valid Accuracy: 0.76278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 (Step 452 / 1695) (Loss=0.53893): 100%|| 113/113 [15:54<00:00,  8.45s/it]\n",
      "Validating... (loss=1.23321): 100%|| 29/29 [02:50<00:00,  5.88s/it]00:07,  7.10s/it]\n",
      "Epoch 5/15 (Step 565 / 1695) (Loss=0.81489): 100%|| 113/113 [15:56<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 565\n",
      "Valid Loss: 0.70866\n",
      "Valid Accuracy: 0.68889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.66739): 100%|| 29/29 [02:50<00:00,  5.87s/it]00:07,  7.15s/it]\n",
      "Epoch 6/15 (Step 678 / 1695) (Loss=0.49435): 100%|| 113/113 [15:55<00:00, 57.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 678\n",
      "Valid Loss: 0.56712\n",
      "Valid Accuracy: 0.77722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 (Step 678 / 1695) (Loss=0.49435): 100%|| 113/113 [15:55<00:00,  8.45s/it]\n",
      "Validating... (loss=0.82345): 100%|| 29/29 [02:50<00:00,  5.86s/it]00:06,  6.78s/it]\n",
      "Epoch 7/15 (Step 791 / 1695) (Loss=0.53851): 100%|| 113/113 [15:54<00:00, 56.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 791\n",
      "Valid Loss: 0.45838\n",
      "Valid Accuracy: 0.82611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 (Step 791 / 1695) (Loss=0.53851): 100%|| 113/113 [15:54<00:00,  8.45s/it]\n",
      "Validating... (loss=0.93572): 100%|| 29/29 [02:50<00:00,  5.87s/it]00:06,  6.93s/it]\n",
      "Epoch 8/15 (Step 904 / 1695) (Loss=0.52666): 100%|| 113/113 [15:54<00:00, 57.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 904\n",
      "Valid Loss: 0.44099\n",
      "Valid Accuracy: 0.82722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 (Step 904 / 1695) (Loss=0.52666): 100%|| 113/113 [15:54<00:00,  8.45s/it]\n",
      "Validating... (loss=0.76807): 100%|| 29/29 [02:51<00:00,  5.90s/it]<00:07,  7.17s/it]\n",
      "Epoch 9/15 (Step 1017 / 1695) (Loss=0.45519): 100%|| 113/113 [15:56<00:00, 57.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1017\n",
      "Valid Loss: 0.43088\n",
      "Valid Accuracy: 0.83444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 (Step 1017 / 1695) (Loss=0.45519): 100%|| 113/113 [15:56<00:00,  8.46s/it]\n",
      "Validating... (loss=0.27453): 100%|| 29/29 [02:50<00:00,  5.86s/it]7<00:06,  6.87s/it]\n",
      "Epoch 10/15 (Step 1130 / 1695) (Loss=0.61595): 100%|| 113/113 [15:57<00:00, 56.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1130\n",
      "Valid Loss: 0.39474\n",
      "Valid Accuracy: 0.83944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 (Step 1130 / 1695) (Loss=0.61595): 100%|| 113/113 [15:57<00:00,  8.47s/it]\n",
      "Validating... (loss=0.31959): 100%|| 29/29 [02:48<00:00,  5.80s/it]5<00:06,  6.82s/it]\n",
      "Epoch 11/15 (Step 1243 / 1695) (Loss=0.37431): 100%|| 113/113 [15:53<00:00, 56.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1243\n",
      "Valid Loss: 0.38341\n",
      "Valid Accuracy: 0.84667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 (Step 1243 / 1695) (Loss=0.37431): 100%|| 113/113 [15:53<00:00,  8.44s/it]\n",
      "Validating... (loss=0.07008): 100%|| 29/29 [02:49<00:00,  5.86s/it]0<00:06,  6.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1356\n",
      "Valid Loss: 0.36632\n",
      "Valid Accuracy: 0.85278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 (Step 1356 / 1695) (Loss=0.50477): 100%|| 113/113 [15:50<00:00,  8.41s/it]\n",
      "Validating... (loss=0.09151): 100%|| 29/29 [02:50<00:00,  5.86s/it]2<00:06,  6.95s/it]\n",
      "Epoch 13/15 (Step 1469 / 1695) (Loss=0.49020): 100%|| 113/113 [15:52<00:00, 56.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1469\n",
      "Valid Loss: 0.37080\n",
      "Valid Accuracy: 0.85389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 (Step 1469 / 1695) (Loss=0.49020): 100%|| 113/113 [15:52<00:00,  8.43s/it]\n",
      "Validating... (loss=0.08488): 100%|| 29/29 [02:49<00:00,  5.84s/it]0<00:06,  6.75s/it]\n",
      "Epoch 14/15 (Step 1582 / 1695) (Loss=0.31477): 100%|| 113/113 [15:50<00:00, 56.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1582\n",
      "Valid Loss: 0.36491\n",
      "Valid Accuracy: 0.86167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 (Step 1582 / 1695) (Loss=0.31477): 100%|| 113/113 [15:50<00:00,  8.41s/it]\n",
      "Validating... (loss=0.08366): 100%|| 29/29 [02:49<00:00,  5.84s/it]3<00:06,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1695\n",
      "Valid Loss: 0.36217\n",
      "Valid Accuracy: 0.86333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 (Step 1695 / 1695) (Loss=0.32732): 100%|| 113/113 [15:53<00:00,  8.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#스케줄러\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\" 학습 초기에는 학습률을 점진적으로 증가시키는 warmup기간을 갖고 그 이후에는 코사인 함수를 따라 학습률을 감소시킴\n",
    "        스케줄러는 optimizer에 설정된 학습률을 기준으로 하여, 특정 시점에서의 학습률을 조절하는 비율(factor)을 계산\n",
    "        warmup 기간 내: step / warmup_steps 비율로 학습률을 증가, 초기 학습률 x step/warmup_steps\n",
    "        warmup 기간 이후: 초기 학습률 x 0.5 * (1 + cos(π * cycles * 2 * progress)) 공식에 따라 학습률이 감소\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def valid(model, writer, test_loader,eval_batch_size, global_step,device):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            logits = model(x)\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    print(\"Global Steps: %d\" % global_step)\n",
    "    print(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    print(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    logger.info(\"/n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Global Steps: %d\" % global_step)\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, output_dir, name):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(output_dir, \"%s_checkpoint.bin\" % name)\n",
    "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
    "    logger.info(\"Saved model checkpoint to [DIR: %s]\", output_dir)\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"logs\", name))\n",
    "\n",
    "train_loader, test_loader = get_loader(224, train_img_dir, train_annotation_dir, test_img_dir, test_annotaion_dir, train_batch_size, eval_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=weight_decay)\n",
    "t_total = num_steps\n",
    "\n",
    "# 스케줄러\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "\n",
    "model.zero_grad()\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "losses = AverageMeter()\n",
    "global_step, best_acc = 0, 0\n",
    "\n",
    "# epoch 단위로 학습 시작\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_iterator = tqdm(train_loader,\n",
    "                          desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        outputs = model(x)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        #loss 계산\n",
    "        loss = loss_fn(outputs, y)\n",
    "        #역전파 학습, 기울기 계산\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0: # 여러 스텝에 걸쳐 gradient를 축적하고 모델에 업데이트, gpu 메모리가 작을 때 큰 배치사이즈를 사용 가능\n",
    "            #loss update\n",
    "            losses.update(loss.item() * gradient_accumulation_steps)\n",
    "            #임계값이 넘는 gradient를 임계값으로 수정, gradient 폭발 방지\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            scheduler.step() # 새로운 학습률을 계산하고 optimizer에 지정되어 있는 학습률 update\n",
    "            optimizer.step() # weight update\n",
    "            optimizer.zero_grad() # 이전 gradient 초기화\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_iterator.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} (Step {global_step} / {t_total}) (Loss={losses.val:.5f})\"\n",
    "            )\n",
    "            writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
    "            writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                accuracy = valid(model, writer, test_loader, eval_batch_size, global_step, device)\n",
    "\n",
    "                if best_acc < accuracy:\n",
    "                    save_model(model, output_dir, name)\n",
    "                    best_acc = accuracy\n",
    "                model.train()\n",
    "    \n",
    "    losses.reset() # 각 epoch의 끝에 loss 초기화\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력층만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#스케줄러\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\" 학습 초기에는 학습률을 점진적으로 증가시키는 warmup기간을 갖고 그 이후에는 코사인 함수를 따라 학습률을 감소시킴\n",
    "        스케줄러는 optimizer에 설정된 학습률을 기준으로 하여, 특정 시점에서의 학습률을 조절하는 비율(factor)을 계산\n",
    "        warmup 기간 내: step / warmup_steps 비율로 학습률을 증가, 초기 학습률 x step/warmup_steps\n",
    "        warmup 기간 이후: 초기 학습률 x 0.5 * (1 + cos(π * cycles * 2 * progress)) 공식에 따라 학습률이 감소\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def valid(model, writer, test_loader,eval_batch_size, global_step,device):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            logits = model(x)\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    print(\"Global Steps: %d\" % global_step)\n",
    "    print(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    print(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    logger.info(\"/n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Global Steps: %d\" % global_step)\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, output_dir, name):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(output_dir, \"%s_checkpoint.bin\" % name)\n",
    "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
    "    logger.info(\"Saved model checkpoint to [DIR: %s]\", output_dir)\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"logs\", name))\n",
    "\n",
    "train_loader, test_loader = get_loader(224, train_img_dir, train_annotation_dir, test_img_dir, test_annotaion_dir, train_batch_size, eval_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                            lr=learning_rate,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=weight_decay)\n",
    "t_total = num_steps\n",
    "\n",
    "# 스케줄러\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "\n",
    "model.zero_grad()\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "losses = AverageMeter()\n",
    "global_step, best_acc = 0, 0\n",
    "\n",
    "# epoch 단위로 학습 시작\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_iterator = tqdm(train_loader,\n",
    "                          desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        outputs = model(x)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        #loss 계산\n",
    "        loss = loss_fn(outputs, y)\n",
    "        #역전파 학습, 기울기 계산\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0: # 여러 스텝에 걸쳐 gradient를 축적하고 모델에 업데이트, gpu 메모리가 작을 때 큰 배치사이즈를 사용 가능\n",
    "            #loss update\n",
    "            losses.update(loss.item() * gradient_accumulation_steps)\n",
    "            #임계값이 넘는 gradient를 임계값으로 수정, gradient 폭발 방지\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            scheduler.step() # 새로운 학습률을 계산하고 optimizer에 지정되어 있는 학습률 update\n",
    "            optimizer.step() # weight update\n",
    "            optimizer.zero_grad() # 이전 gradient 초기화\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_iterator.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} (Step {global_step} / {t_total}) (Loss={losses.val:.5f})\"\n",
    "            )\n",
    "            writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
    "            writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                accuracy = valid(model, writer, test_loader, eval_batch_size, global_step, device)\n",
    "\n",
    "                if best_acc < accuracy:\n",
    "                    save_model(model, output_dir, name)\n",
    "                    best_acc = accuracy\n",
    "                model.train()\n",
    "    \n",
    "    losses.reset() # 각 epoch의 끝에 loss 초기화\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mlp_mixer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
