{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\key_bert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "tokenizer.encode(\"한국어 모델을 공유합니다.\")\n",
    "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[SEP]', '생존', '안정', '멸', '발전']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# KoBERT 모델과 토크나이저 불러오기\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1',output_attentions=True)\n",
    "\n",
    "# 문서 예제\n",
    "text = \"국가와 기업은 밀접한 상호작용을 통해 서로에게 큰 영향을 미치고 있습니다. 기업이 성공적으로 성장하면 국가는 경제적 안정과 발전을 경험할 수 있습니다. 따라서 국가 경제를 발전시키기 위해서는 기업의 창립과 지속이 매우 중요한 과제인데요. 통계청의 ‘2022년 기업생멸행정통계’​를 통해  우리나라 기업의 생존율을 알아보도록 하겠습니다!\"\n",
    "# 토큰화 및 토큰 ID로 변환\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# 모델을 사용하여 출력 계산\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor)\n",
    "\n",
    "# Attention 값 추출\n",
    "attention = outputs.attentions  # 각 레이어와 헤드별 Attention 값이 담긴 리스트\n",
    "\n",
    "# 마지막 레이어의 Attention 값 평균 계산\n",
    "last_layer_attention = attention[-1]  # 마지막 레이어의 Attention\n",
    "mean_attention = last_layer_attention.mean(dim=1)  # 헤드에 대한 평균\n",
    "\n",
    "# 토큰별로 평균 Attention Score 계산\n",
    "token_attention_scores = mean_attention.squeeze().mean(dim=0)\n",
    "\n",
    "# Attention Score가 가장 높은 토큰 추출\n",
    "top_tokens = token_attention_scores.topk(5)  # 상위 5개 토큰\n",
    "\n",
    "# 추출된 키워드 출력\n",
    "keywords = [tokenizer.decode([input_ids[idx]]) for idx in top_tokens.indices]\n",
    "print(keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 어텐션 스코어 추출\u001b[39;00m\n\u001b[0;32m     21\u001b[0m attention \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mattentions\n\u001b[1;32m---> 22\u001b[0m last_layer_attention \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m mean_attention \u001b[38;5;241m=\u001b[39m last_layer_attention\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m token_attention_scores \u001b[38;5;241m=\u001b[39m mean_attention\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# KoBERT 모델과 토크나이저 불러오기\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "# 문서 예제\n",
    "text = \"여기에 분석하고자 하는 문서를 넣으세요. 예를 들어, 이 문장은 BERT 모델을 사용하여 키워드를 추출하는 예제입니다.\"\n",
    "\n",
    "# 토큰화 및 토큰 ID로 변환\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# 모델을 사용하여 출력 계산\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor)\n",
    "\n",
    "# 어텐션 스코어 추출\n",
    "attention = outputs.attentions\n",
    "last_layer_attention = attention[-1]\n",
    "mean_attention = last_layer_attention.mean(dim=1)\n",
    "token_attention_scores = mean_attention.squeeze().mean(dim=0).numpy()\n",
    "\n",
    "# TF-IDF 계산\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform([text])\n",
    "tfidf_scores = np.array(tfidf.sum(axis=0)).flatten()\n",
    "\n",
    "# 단어 사전 구축\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# 어텐션 스코어와 TF-IDF 스코어 결합\n",
    "combined_scores = token_attention_scores * tfidf_scores\n",
    "\n",
    "# 결합된 스코어를 기반으로 상위 키워드 추출\n",
    "num_keywords = 5\n",
    "top_indices = combined_scores.argsort()[-num_keywords:]\n",
    "keywords = [feature_names[i] for i in top_indices]\n",
    "\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "key_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
