{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI HUB의 문서 요약 데이터로 fine tuning된 weight로 요약한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 텍스트: 서울특별시 종로구의 조선왕조 법궁인 경복궁의 남쪽에 있는 정문인 '광화문'은 '임금의 큰 덕(德)이 온 나라를 비춘다'는 의미로, 1395년에 세워졌으며, 2층 누각 구조로 되어 있다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart import BartForConditionalGeneration \n",
    "\n",
    "# 모델 바이너리 파일 경로\n",
    "#model_binary_path = '/content/drive/MyDrive/KoBART-summarization-main/KoBART-summarization-main/kobart_summary'\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')\n",
    "\n",
    "input_text =  \"\"\"\n",
    "광화문(光化門)은 서울특별시 종로구의 조선왕조 법궁인 경복궁의 남쪽에 있는 정문이다. \"임금의 큰 덕(德)이 온 나라를 비춘다\"는 의미이다. 1395년에 세워졌으며, 2층 누각 구조로 되어 있다. 경복궁의 정전인 근정전으로 가기 위해 지나야 하는 문 3개 중에서 첫째로 마주하는 문이며, 둘째는 흥례문, 셋째는 근정문이다. 광화문 앞에는 지금은 도로 건설로 사라진 월대가 자리잡고 있었으며, 양쪽에는 한 쌍의 해태 조각상이 자리잡고 있다. 광화문의 석축부에는 세 개의 홍예문(虹霓門, 아치문)이 있다. 가운데 문은 임금이 다니던 문이고, 나머지 좌우의 문은 신하들이 다니던 문이었는데, 왼쪽 문은 무신이, 오른쪽 문은 문신이 출입했다. 광화문의 가운데 문 천장에는 주작이 그려져 있고, 왼쪽 문에는 거북이가, 오른쪽 문에는 천마가 그려져 있다.\n",
    "\"\"\"\n",
    "# 입력 텍스트를 토큰화하여 인코딩\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 모델을 사용하여 요약 생성\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 요약 결과 디코딩\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 요약 출력\n",
    "print(\"요약 텍스트:\", summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 텍스트: 서울특별시 종로구의 조선왕조 법궁인 경복궁의 남쪽에 있는 정문인 광화문(光化門)은 서울특별시 종로구의 조선왕조 법궁인 경복궁의 남쪽에 있는 정문이다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart import BartForConditionalGeneration \n",
    "\n",
    "# 모델 바이너리 파일 경로\n",
    "#model_binary_path = '/content/drive/MyDrive/KoBART-summarization-main/KoBART-summarization-main/kobart_summary'\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "\n",
    "input_text =  \"\"\"\n",
    "광화문(光化門)은 서울특별시 종로구의 조선왕조 법궁인 경복궁의 남쪽에 있는 정문이다. \"임금의 큰 덕(德)이 온 나라를 비춘다\"는 의미이다. 1395년에 세워졌으며, 2층 누각 구조로 되어 있다. 경복궁의 정전인 근정전으로 가기 위해 지나야 하는 문 3개 중에서 첫째로 마주하는 문이며, 둘째는 흥례문, 셋째는 근정문이다. 광화문 앞에는 지금은 도로 건설로 사라진 월대가 자리잡고 있었으며, 양쪽에는 한 쌍의 해태 조각상이 자리잡고 있다. 광화문의 석축부에는 세 개의 홍예문(虹霓門, 아치문)이 있다. 가운데 문은 임금이 다니던 문이고, 나머지 좌우의 문은 신하들이 다니던 문이었는데, 왼쪽 문은 무신이, 오른쪽 문은 문신이 출입했다. 광화문의 가운데 문 천장에는 주작이 그려져 있고, 왼쪽 문에는 거북이가, 오른쪽 문에는 천마가 그려져 있다.\n",
    "\"\"\"\n",
    "# 입력 텍스트를 토큰화하여 인코딩\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 모델을 사용하여 요약 생성\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 요약 결과 디코딩\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 요약 출력\n",
    "print(\"요약 텍스트:\", summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test_data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\개인및관계.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\미용과건강.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\상거래(쇼핑).json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\시사교육.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\식음료.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\여가생활.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\일과직업.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\주거와생활.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Training\\\\[라벨]한국어대화요약_train\\\\[라벨]한국어대화요약_train\\\\행사.json']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "file_list= glob.glob('C:/Users/USER/Downloads/한국어 대화 요약/Training/[[]라벨[]]한국어대화요약_train/[[]라벨[]]한국어대화요약_train/*.json')\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def train_test_data_load(file):\n",
    "    with open(file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    documents = []\n",
    "    summaries = []\n",
    "\n",
    "    for document in json_data['data']:\n",
    "        one_document = ' '.join([re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9 ]', '', sentence['utterance']) for sentence in document['body']['dialogue'] if sentence])\n",
    "        documents.append(one_document)\n",
    "        summaries.append(re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9 ]', '', document['body']['summary']))\n",
    "\n",
    "    split = int(len(json_data['data']) * 0.8)\n",
    "    train_docu = documents[:3000]\n",
    "    train_sum = summaries[:3000]\n",
    "    test_docu = documents[3000:6000]\n",
    "    test_sum = summaries[3000:6000]\n",
    "\n",
    "    train_docu_sum_df = pd.DataFrame(data=list(zip(train_docu, train_sum)), columns=['documents', 'summary'])\n",
    "    test_docu_sum_df = pd.DataFrame(data=list(zip(test_docu, test_sum)), columns=['documents', 'summary'])\n",
    "    print(len(train_docu_sum_df), len(test_docu_sum_df))\n",
    "    \n",
    "    return train_docu_sum_df, test_docu_sum_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26298"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_list[2],'r') as f:\n",
    "    json_data= json.load(f)\n",
    "\n",
    "len(json_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n",
      "3000 3000\n",
      "3000 3000\n",
      "3000 3000\n",
      "3000 3000\n",
      "3000 3000\n",
      "3000 3000\n",
      "3000 3000\n",
      "3000 3000\n"
     ]
    }
   ],
   "source": [
    "train_df_list=[]\n",
    "test_df_list=[]\n",
    "\n",
    "for file in file_list:\n",
    "    train_df, test_df= train_test_data_load(file)\n",
    "    train_df_list.append(train_df)\n",
    "    test_df_list.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df=pd.DataFrame()\n",
    "final_test_df= pd.DataFrame()\n",
    "for df in train_df_list:\n",
    "    final_train_df= pd.concat([final_train_df, df], axis=0)\n",
    "final_train_df.reset_index(drop=True, inplace=True)\n",
    "final_train_df= final_train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "for df in test_df_list:\n",
    "    final_test_df= pd.concat([final_test_df,df], axis=0)\n",
    "\n",
    "\n",
    "final_test_df.reset_index(drop=True, inplace=True)\n",
    "final_test_df= final_test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "documents    0\n",
       "summary      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'담주에 이름 오는날이자나 20이엇나 21이엇나 뭉치쟈 헐 대박 벌써 다음주야완전 좋다아아 다같이 모이자ㅋㅋㅋㅋㅋㅋ 근데 아까무러보니깐 오는 주에는 안되는가바 1월말에보쟈던데ㅠㅠㅠㅠ 모허닝 집이뉘ㅣㅣ 헉 글쿠나ㅠㅠ 1월 말에 날잡아서 보쟈 앙앙 집이얌 ㅎㅅㅎㅎㅎㅎ'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_df['documents'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document 통계값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 수에 대한 통계:\n",
      "count    27000.000000\n",
      "mean       123.439370\n",
      "std         55.695231\n",
      "min         28.000000\n",
      "25%         86.000000\n",
      "50%        111.000000\n",
      "75%        147.000000\n",
      "max        903.000000\n",
      "Name: doc_length_chars, dtype: float64\n",
      "\n",
      "단어 수에 대한 통계:\n",
      "count    27000.000000\n",
      "mean        27.973111\n",
      "std         13.259013\n",
      "min          7.000000\n",
      "25%         19.000000\n",
      "50%         25.000000\n",
      "75%         34.000000\n",
      "max        240.000000\n",
      "Name: doc_length_words, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 가정: 이미 DataFrame이 'df'로 주어져 있음\n",
    "# df의 구조: columns=['documents', 'summary']\n",
    "\n",
    "# 문자 수 계산\n",
    "final_train_df['doc_length_chars'] = final_train_df['documents'].apply(len)\n",
    "\n",
    "# 단어 수 계산: 공백을 기준으로 분리된 단어의 수 계산\n",
    "final_train_df['doc_length_words'] = final_train_df['documents'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 통계값 확인\n",
    "# 문자 수에 대한 통계값\n",
    "stats_chars = final_train_df['doc_length_chars'].describe()\n",
    "\n",
    "\n",
    "# 단어 수에 대한 통계값\n",
    "stats_words = final_train_df['doc_length_words'].describe()\n",
    "\n",
    "print(\"문자 수에 대한 통계:\")\n",
    "print(stats_chars)\n",
    "\n",
    "print(\"\\n단어 수에 대한 통계:\")\n",
    "print(stats_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valid_data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\개인및관계.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\미용과건강.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\상거래(쇼핑).json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\시사교육.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\식음료.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\여가생활.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\일과직업.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\주거와생활.json',\n",
       " 'C:/Users/USER/Downloads/한국어 대화 요약/Validation\\\\[라벨]한국어대화요약_valid\\\\[라벨]한국어대화요약_valid\\\\행사.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "file_list= glob.glob('C:/Users/USER/Downloads/한국어 대화 요약/Validation/[[]라벨[]]한국어대화요약_valid/[[]라벨[]]한국어대화요약_valid/*.json')\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def data_load(file):\n",
    "\n",
    "\n",
    "    with open(file,'r') as f:\n",
    "        json_data= json.load(f)\n",
    "\n",
    "\n",
    "    documents = []\n",
    "    summary = []\n",
    "    for document in json_data['data'][:1000]:\n",
    "        one_document = ''\n",
    "        summary.append(re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9 ]','',document['body']['summary']))\n",
    "        for sentence in document['body']['dialogue']:\n",
    "            # 비어있는 리스트인지 확인\n",
    "            if sentence:  # sentences 리스트가 비어있지 않다면\n",
    "                one_document += ' ' + sentence['utterance']\n",
    "        documents.append(re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9 ]','', one_document.strip()))  # strip()을 사용하여 맨 앞의 줄바꿈 문자를 제거\n",
    "    docu_summ_df= pd.DataFrame(data=list(zip(documents, summary)), columns=['documents','summary'])\n",
    "    return docu_summ_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[]\n",
    "\n",
    "for file in file_list:\n",
    "    df= data_load(file)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.DataFrame()\n",
    "for df in df_list:\n",
    "    final_df= pd.concat([final_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.reset_index(drop=True, inplace=True)\n",
    "final_df= final_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 생성 및 디렉토리 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/data/train2.tsv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "final_test_df.to_csv('train2.tsv', sep='\\t', encoding=\"utf-8\", index=False)\n",
    "\n",
    "# 원본 파일 경로\n",
    "source_file = 'C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/train2.tsv'\n",
    "# 이동할 위치의 폴더 경로\n",
    "destination_folder = 'C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/data/train2.tsv'\n",
    "# 파일 이동\n",
    "shutil.copyfile(source_file, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/data/validation2.tsv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "final_df.to_csv('validation2.tsv', sep='\\t', encoding=\"utf-8\", index=False)\n",
    "\n",
    "# 원본 파일 경로\n",
    "source_file = 'C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/validation2.tsv'\n",
    "# 이동할 위치의 폴더 경로\n",
    "destination_folder = 'C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/data/validation2.tsv'\n",
    "# 파일 이동\n",
    "shutil.copyfile(source_file, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\key_bert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "\u001b[32m2024-05-04 08:37:09.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mNamespace(accelerator='gpu', batch_size=40, checkpoint='checkpoint4', gradient_clip_val=1.0, lr=3e-05, max_epochs=10, max_len=200, num_gpus=1, num_workers=4, test_file='data/validation.tsv', train_file='data/train.tsv')\u001b[0m\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchoiyw\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/choiyw/KoBART-summ4/runs/767mhyqr\" target=\"_blank\">firm-meadow-2</a></strong> to <a href=\"https://wandb.ai/choiyw/KoBART-summ4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\USER\\anaconda3\\envs\\key_bert\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\USER\\anaconda3\\envs\\key_bert\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 123 M \n",
      "-------------------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.440   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\key_bert\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\key_bert\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:590: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5600/5600 [1:56:05<00:00,  0.80it/s, v_num=hyqr, train_loss=2.120, val_loss=2.080]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 5600: 'val_loss' reached 2.08333 (best 2.08333), saving model to 'C:\\\\Users\\\\USER\\\\Desktop\\\\text_mining_project2\\\\KoBART-summarization-main\\\\checkpoint4\\\\model_chp/epoch=00-val_loss=2.083.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5600/5600 [1:56:25<00:00,  0.80it/s, v_num=hyqr, train_loss=1.850, val_loss=1.850]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 11200: 'val_loss' reached 1.85030 (best 1.85030), saving model to 'C:\\\\Users\\\\USER\\\\Desktop\\\\text_mining_project2\\\\KoBART-summarization-main\\\\checkpoint4\\\\model_chp/epoch=01-val_loss=1.850.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 5600/5600 [1:56:33<00:00,  0.80it/s, v_num=hyqr, train_loss=1.630, val_loss=1.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 16800: 'val_loss' reached 1.81464 (best 1.81464), saving model to 'C:\\\\Users\\\\USER\\\\Desktop\\\\text_mining_project2\\\\KoBART-summarization-main\\\\checkpoint4\\\\model_chp/epoch=02-val_loss=1.815.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5600/5600 [1:56:35<00:00,  0.80it/s, v_num=hyqr, train_loss=1.410, val_loss=1.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 22400: 'val_loss' reached 1.80857 (best 1.80857), saving model to 'C:\\\\Users\\\\USER\\\\Desktop\\\\text_mining_project2\\\\KoBART-summarization-main\\\\checkpoint4\\\\model_chp/epoch=03-val_loss=1.809.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 5600/5600 [1:56:21<00:00,  0.80it/s, v_num=hyqr, train_loss=1.470, val_loss=1.820]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 28000: 'val_loss' reached 1.81986 (best 1.80857), saving model to 'C:\\\\Users\\\\USER\\\\Desktop\\\\text_mining_project2\\\\KoBART-summarization-main\\\\checkpoint4\\\\model_chp/epoch=04-val_loss=1.820.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 5600/5600 [1:56:18<00:00,  0.80it/s, v_num=hyqr, train_loss=1.270, val_loss=1.840]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 33600: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 5600/5600 [2:02:30<00:00,  0.76it/s, v_num=hyqr, train_loss=1.160, val_loss=1.860]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 39200: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 5600/5600 [4:33:03<00:00,  0.34it/s, v_num=hyqr, train_loss=1.190, val_loss=1.890]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 44800: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 5600/5600 [1:56:02<00:00,  0.80it/s, v_num=hyqr, train_loss=0.943, val_loss=1.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 50400: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5600/5600 [1:56:02<00:00,  0.80it/s, v_num=hyqr, train_loss=1.120, val_loss=1.920]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 56000: 'val_loss' was not in top 3\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5600/5600 [1:56:07<00:00,  0.80it/s, v_num=hyqr, train_loss=1.120, val_loss=1.920]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from dataset import KobartSummaryModule\n",
    "from model import KoBARTConditionalGeneration\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# argparse.Namespace를 사용하여 파라미터 설정\n",
    "args = argparse.Namespace(\n",
    "    train_file='data/train.tsv',\n",
    "    test_file='data/validation.tsv',\n",
    "    batch_size=40,\n",
    "    checkpoint='checkpoint4',\n",
    "    max_len=200,\n",
    "    max_epochs=10,\n",
    "    lr=3e-5,\n",
    "    accelerator='gpu',\n",
    "    num_gpus=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    num_workers=4,  # 필요에 따라 조정\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "logger.info(args)\n",
    "\n",
    "dm = KobartSummaryModule(args.train_file,\n",
    "                         args.test_file,\n",
    "                         tokenizer,\n",
    "                         batch_size=args.batch_size,\n",
    "                         max_len=args.max_len,\n",
    "                         num_workers=args.num_workers)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "model = KoBARTConditionalGeneration(args)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                       dirpath=args.checkpoint,\n",
    "                                       filename='model_chp/{epoch:02d}-{val_loss:.3f}',\n",
    "                                       verbose=True,\n",
    "                                       save_last=True,\n",
    "                                       mode='min',\n",
    "                                       save_top_k=3)\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"KoBART-summ4\")\n",
    "\n",
    "\n",
    "trainer = L.Trainer(max_epochs=args.max_epochs,\n",
    "                     accelerator=args.accelerator,\n",
    "                     devices=args.num_gpus if args.accelerator == 'gpu' else None,\n",
    "                     gradient_clip_val=args.gradient_clip_val,\n",
    "                     callbacks=[checkpoint_callback],\n",
    "                     logger=wandb_logger)\n",
    "\n",
    "trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_model_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from train import KoBARTConditionalGeneration\n",
    "from transformers.models.bart import BartForConditionalGeneration\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    hparams=None,\n",
    "    model_binary='C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/checkpoint4/model_chp/epoch=03-val_loss=1.809.ckpt',\n",
    "    output_dir='kobart_summary3'\n",
    ")\n",
    "\n",
    "\n",
    "inf = KoBARTConditionalGeneration.load_from_checkpoint(args.model_binary)\n",
    "\n",
    "inf.model.save_pretrained(args.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference_fine tuning weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df= pd.read_csv('C:/Users/USER/Desktop/text_mining_project2/KoBART-summarization-main/test.tsv', sep='\\t')\n",
    "test_df= test_df.sample(2000, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 텍스트: 새 신발에 발이 쓸리면 아프지만 마라탕이 먹고 강사님이 밥 같이 먹으라고 했다고 이야기하고 대화 상대자는 강사랑을 같이 먹으라고 했다고 이야기하고 있다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart import BartForConditionalGeneration \n",
    "\n",
    "# 모델 바이너리 파일 경로\n",
    "#model_binary_path = '/content/drive/MyDrive/KoBART-summarization-main/KoBART-summarization-main/kobart_summary'\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "model = BartForConditionalGeneration.from_pretrained('kobart_summary2')\n",
    "summary_texts=[]\n",
    "#input_text =  test_df['documents'][0]\n",
    "# 입력 텍스트를 토큰화하여 인코딩\n",
    "input_texts= test_df['documents']\n",
    "for input_text in input_texts:\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # 모델을 사용하여 요약 생성\n",
    "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=1.0, num_beams=8, early_stopping=True)\n",
    "\n",
    "    # 요약 결과 디코딩\n",
    "    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summary_texts.append(summary_text)\n",
    "# 요약 출력\n",
    "print(\"요약 텍스트:\", summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.36729227573137385, 'p': 0.27774689202229386, 'r': 0.5875876003660577}, 'rouge-l': {'f': 0.3092668345760144, 'p': 0.2332867227991115, 'r': 0.49779812764817954}}\n"
     ]
    }
   ],
   "source": [
    "from rouge_metric import Rouge\n",
    "metrics = [\"rouge-n\",'rouge-l']\n",
    "rouge = Rouge(metrics=metrics, max_n=1)\n",
    "score= rouge.get_scores(summary_texts, test_df['summary'].to_list())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 지금 선생님께서 1500만원 있으신 걸 저희가 1000만원을 더 올려가지고 최고 2500만원 최저 25만원을 받아가실 수 있게끔 해드리고 두 번째가 운전 중에 교통사고 처리 지원금이라고 해서 우리 고객님 사고로 인해서 다른 사람을 대인 피해가 있을 때 우리 고객님 요금이 1인당 합의금을  100000000까지 한도로 넣어드리는데 지금 현재 30000000원 밖에 없으시기 때문에 저희가 70000000원을 올려드린다는 거예요. 예. 그리고요. 또 1가지가 요것도 이제 교통사고 처리 지원금이라고 해서요. 우리 고객님께서 할인을 보시는데 법규 중대 법규 위반으로 해서 6주 미만 사고 시에 저희가 요거 5000000원이 들어가요. 요거 없으시거든요. 그리고 운증 중 사고 벌금이요.  인사사고가 났을 때 스쿨존의 사고 시에는 가중처벌받는 걸로 되면서 벌금이 10000000원이 더 올라갔어요. 그래서 요거 스쿨존 사고 시 10000000원을 넣어드릴 거고요. 그리고 운전 중 사고 벌금이 있어요. 그러면 벌금은 대인벌금이 있고 대물벌금이 있는데 대물벌금이 선생님 빠져있는 부분 저희가 5000000원을 넣어드릴 거예요.  네, 그리고요. 또 변호사를 선임을 하실 때 지금 현재 5000000원 밖에 가입이 안 되어 있는데 지금 물가 상승 때문에 변호사 선임 5000000원으로는 안 되잖아요. 그래서 저희가 한도가 2000이에요. 요거 모자라는 15000000원을 넣어드릴 거고요.  그리고 요즘 운전 중에 보복운전자 많잖아요. 그러다 보니까 보복운전으로 해서 피해 보장을 저희가 70만원 이거 넣어드리는 거거든요. 이렇게 빠져있는 부분 다 싹 다 넣어드리니까 5,140원으로 선생님 완벽하게 보완이 들어가시는 거예요. 이게 패키지명이 어떻게 되죠? 이게? 네? 선생님? 이렇게 해가지고 추가하는 거에 대한 걸 뭐라고 해요? 이거를 뭐  선생님 요거 추가하는 거는요 빠져있는 부분을 저희가 넣어드리는 부분이기 때문에 선생님께서 지금 현재 마일리지 운전자 보험이 2019년도 9월달에 가입을 하셨던 게 있으시거든요 여기 보험은요 수급 대수가 안 돼요 왜냐하면 보험 날짜가 달라지잖아요 보험 날짜가 틀어지기 때문에 요거는 그대로 우리 고객님 유지하시면서 모자라는 부분만 오늘부터 넣어드리면 그래서 더 들어가는 거네요  네, 오늘부터 해가지고. 오늘부터 요게 보장을 그러면 받아가시는데 다른 보험은 우리 고객님 가입하시고 나서 90일이 지나서 보장을 받아가시고 50% 받아가고 이렇게 되는 거잖아요. 운전자 보험은요. 요거 바로 실시간 보장을 해드리는 거예요. 그러면 만약에 빠져있는 부분은 오늘 넣으셨는데 그럴 일은 없어야 돼요. 혹여라도 내가 사고로 인해서 요거 보장이 필요하다 그러시면요. 오늘부터 요거는 100% 보장을 해드리는 거예요.  그렇게 할 일이 없겠지만 보통은 알겠습니다. 그러면 괜찮은 것 같아요. 그렇게 7가지 정도로 해가지고 고장이 쉬는데 한 5,140원이면 뭘 해가지고 이분이 이렇게 해놓으셔야 완벽하시니까요. 선생님 제가 빨리 녹음으로 가름을 해드릴게요. 왜냐하면 선생님께서 좋아, 오케이 해주세요라고 하셔도 저희가 이게 녹음으로 전부 다  가름이 되어야지 우리 선생님께서 법적으로 보장을 받으실 수가 있고 저도 보호를 받는 거거든요. 그래서 선생님 제가 빨리 한번 읽어드릴게요. 선생님 답변 좀 부탁을 드리겠습니다. 지금 통화 중이신 우리 고객님께서는 최희자 범차 선생님이시고요. 저는 악사 손해보험사의 효령의 김혜란입니다.  저희가 계약 상담과 실손담보 중복 확인 및 인수 여부 결정을 위해서 우리 고객님 동의 녹취가 필요하고요. 동의를 하셨더라도 추후에 중지 요청 가능하시고요. 거구 시에는 정상적인 상담이 어렵습니다. 최희철 검사님이 주민번호와 질병 상해 정보 등을 수집, 활용하고 가입하신 보험 계약 및 보험금 지급 정보를 한국신용정보공 개발원 등에 3개월간 조회하여 1년간 보유하는데 동의하십니까?  네, 주민번호와 같은 고유 세대 정보 및 질병 상해 정보를 말씀을 드린 대로 처리하는데 동의하십니까? 네. 네, 선생님 저희가요. 운전자 보험도 실비 보험하고 동일해서요. 이게 다른 보험회사에 추가로 이게 가입이 되어있다 그러시면요. 저희가 가입을 못 해드리거든요. 제가 한번 확인을 해드릴게요. 그러면은. 네, 확인하고 제가 말씀을 드릴게요. 네, 확인하겠습니다.  선생님께서요. 지금 대인 배상 있죠? 삼성생명에 죄송합니다. 삼성화재로 해서요. 선생님 지금 교통사고 벌금 대인 있죠? 네. 스쿨존 사고 시 요거 2000만원을 추가해서 1000만원 추가해서 3000만원이 들어가 있어요. 예. 그러시면은 제가  스쿨존스하고 1000만원 추가하는 거는 빼야 됩니다. 이거는 제외해드리고요. 또 한 번 봐드려 보면은 변호사 스님 비용이 있죠. 선생님. 500만원 이하 있었던 거. 네. 1500만원 말씀드렸던 게 변호사 스님 비용 2000만원이 삼성화재로 가입이 되어 있으세요. 그럼 이미 다 돼 있었네요.  되어 있으시고요. 나머지는 제가 한번 봐드려 볼게요. 선생님 6주 미만은 삼성화재에 그게 아직까지 가입이 안 돼요. 선생님. 그래서 6주 미만 사고하고 대물 있죠? 대물도 제가 한번 봐드려 보겠습니다. 네. 이게 있으시면 안 되니까 대물 5배 제가 한번 봐드리 보면은  중복되는 거는 아예 가입을 하셔도 이게 중복 보장이 나가는 게 아니기 때문에 아예 저희가 제외하고 안내를 해드려야 되는 부분이라서요. 선생님 한번 봐드려 보겠습니다. 대물 배상 그리고 변호사 선생님 비용 그리고 보복 운전도 없으세요. 그러시고 30000000원 한도 이거 빼서 한번 봐드려 볼게요. 선생님 이거를 빼고요. 선생님 혹시 운전을 주말에 많이 하세요?  주말에 사서? 금요일 오후부터요. 금, 토, 일을 신주말이라고 얘기를 하는데요. 신주말에 우리 고객님 사고가 나시게 되면 입원하시는 부분 있잖아요. 입원 일당. 우리 고객님 신주말 교통사항에 입원 일당이 저희가 요거 20000원 넣어드려 볼게요.  아니요. 괜찮아요. 보니까 삼성화재에 들어가 있는 게 여러 가지가 있는 것 같네요. 그러면 더 이상 투과할 필요는 없는 것 같아요. 그러면 선생님 제가 이걸 말씀을 드려볼게요. 그러면 교통사고처리지원금이라고 해서 중대법규 위반했을 때 6주 미만은 선생님 이게 저희 악사에서 가입이 가능한 거라서 이거는 삼성 쪽에 보장이 안 들어가 있어요.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('C:/Users/USER/Downloads/result4.csv')\n",
    "document= ' '.join(df['text'][0:100].to_list())\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 수 계산: 공백을 기준으로 분리된 단어의 수 계산\n",
    "df\n",
    "a= document.split(' ')\n",
    "# 단어 수에 대한 통계값\n",
    "\n",
    "\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>22.444</td>\n",
       "      <td>지금 선생님께서 1500만원 있으신 걸 저희가 1000만원을 더 올려가지고 최고 ...</td>\n",
       "      <td>[{'word': '지금', 'start': 0.009, 'end': 0.049, ...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.677</td>\n",
       "      <td>30.921</td>\n",
       "      <td>100000000까지 한도로 넣어드리는데 지금 현재 30000000원 밖에 없으시...</td>\n",
       "      <td>[{'word': '100000000까지', 'start': 23.677, 'end...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32.322</td>\n",
       "      <td>32.382</td>\n",
       "      <td>예.</td>\n",
       "      <td>[{'word': '예.', 'start': 32.322, 'end': 32.382...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32.542</td>\n",
       "      <td>33.082</td>\n",
       "      <td>그리고요.</td>\n",
       "      <td>[{'word': '그리고요.', 'start': 32.542, 'end': 33....</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33.503</td>\n",
       "      <td>38.686</td>\n",
       "      <td>또 1가지가 요것도 이제 교통사고 처리 지원금이라고 해서요.</td>\n",
       "      <td>[{'word': '또', 'start': 33.503, 'end': 33.603,...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>636</td>\n",
       "      <td>2631.077</td>\n",
       "      <td>2633.378</td>\n",
       "      <td>악사손해보험사의 김혜란이었고요.</td>\n",
       "      <td>[{'word': '악사손해보험사의', 'start': 2631.077, 'end'...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>637</td>\n",
       "      <td>2633.638</td>\n",
       "      <td>2634.699</td>\n",
       "      <td>문자 하나 남겨드릴게요.</td>\n",
       "      <td>[{'word': '문자', 'start': 2633.638, 'end': 2633...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>638</td>\n",
       "      <td>2634.759</td>\n",
       "      <td>2635.299</td>\n",
       "      <td>선생님.</td>\n",
       "      <td>[{'word': '선생님.', 'start': 2634.759, 'end': 26...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>639</td>\n",
       "      <td>2635.539</td>\n",
       "      <td>2636.620</td>\n",
       "      <td>수고하세요.</td>\n",
       "      <td>[{'word': '수고하세요.', 'start': 2635.539, 'end': ...</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>640</td>\n",
       "      <td>2636.960</td>\n",
       "      <td>2637.961</td>\n",
       "      <td>네, 감사합니다.</td>\n",
       "      <td>[{'word': '네,', 'start': 2636.96, 'end': 2637....</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>641 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     start       end  \\\n",
       "0             0     0.009    22.444   \n",
       "1             1    23.677    30.921   \n",
       "2             2    32.322    32.382   \n",
       "3             3    32.542    33.082   \n",
       "4             4    33.503    38.686   \n",
       "..          ...       ...       ...   \n",
       "636         636  2631.077  2633.378   \n",
       "637         637  2633.638  2634.699   \n",
       "638         638  2634.759  2635.299   \n",
       "639         639  2635.539  2636.620   \n",
       "640         640  2636.960  2637.961   \n",
       "\n",
       "                                                  text  \\\n",
       "0     지금 선생님께서 1500만원 있으신 걸 저희가 1000만원을 더 올려가지고 최고 ...   \n",
       "1     100000000까지 한도로 넣어드리는데 지금 현재 30000000원 밖에 없으시...   \n",
       "2                                                   예.   \n",
       "3                                                그리고요.   \n",
       "4                    또 1가지가 요것도 이제 교통사고 처리 지원금이라고 해서요.   \n",
       "..                                                 ...   \n",
       "636                                  악사손해보험사의 김혜란이었고요.   \n",
       "637                                      문자 하나 남겨드릴게요.   \n",
       "638                                               선생님.   \n",
       "639                                             수고하세요.   \n",
       "640                                          네, 감사합니다.   \n",
       "\n",
       "                                                 words     speaker  \n",
       "0    [{'word': '지금', 'start': 0.009, 'end': 0.049, ...  SPEAKER_01  \n",
       "1    [{'word': '100000000까지', 'start': 23.677, 'end...  SPEAKER_01  \n",
       "2    [{'word': '예.', 'start': 32.322, 'end': 32.382...  SPEAKER_01  \n",
       "3    [{'word': '그리고요.', 'start': 32.542, 'end': 33....  SPEAKER_01  \n",
       "4    [{'word': '또', 'start': 33.503, 'end': 33.603,...  SPEAKER_01  \n",
       "..                                                 ...         ...  \n",
       "636  [{'word': '악사손해보험사의', 'start': 2631.077, 'end'...  SPEAKER_01  \n",
       "637  [{'word': '문자', 'start': 2633.638, 'end': 2633...  SPEAKER_01  \n",
       "638  [{'word': '선생님.', 'start': 2634.759, 'end': 26...  SPEAKER_01  \n",
       "639  [{'word': '수고하세요.', 'start': 2635.539, 'end': ...  SPEAKER_00  \n",
       "640  [{'word': '네,', 'start': 2636.96, 'end': 2637....  SPEAKER_01  \n",
       "\n",
       "[641 rows x 6 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                   13\n",
       "start                                                   184.253\n",
       "end                                                     191.358\n",
       "text                테스트 데이터 수집을 먼저 해놓고 전체를 시각화하는 것을 진행하려고 하는데요.\n",
       "words         [{'word': '테스트', 'start': 184.253, 'end': 184....\n",
       "speaker                                              SPEAKER_00\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 텍스트: 보험에 가입을 하면 손해 보상을 받을 수 있다고 이야기하고 보험료가 얼마인지 이야기하고 보험료가 얼마인지에 대해 이야기하고있다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart import BartForConditionalGeneration \n",
    "\n",
    "# 모델 바이너리 파일 경로\n",
    "#model_binary_path = '/content/drive/MyDrive/KoBART-summarization-main/KoBART-summarization-main/kobart_summary'\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "model = BartForConditionalGeneration.from_pretrained('kobart_summary2')\n",
    "summary_texts=[]\n",
    "#input_text =  test_df['documents'][0]\n",
    "# 입력 텍스트를 토큰화하여 인코딩\n",
    "\n",
    "input_text= document\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 모델을 사용하여 요약 생성\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=1.0, num_beams=4)\n",
    "\n",
    "# 요약 결과 디코딩\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "summary_texts.append(summary_text)\n",
    "# 요약 출력\n",
    "print(\"요약 텍스트:\", summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여보세요. \\n',\n",
       " '네 안녕하세요 최용우님 맞으시죠 네 여기는 딥러닝 부트 캠프 신청해주신 알파코인데요 네 잠시 통화 가능하세요 네 저희 과정 지원해주셔서 감사드리고요. 저희 지원서 바탕으로 몇 가지 여쭤보고 면접 예약 도와드릴게요 네 어 내일 배움 카드는 있다고 체크해 주셨는데 네 네 실물 카드로 갖고 계신 거죠? 네 혹시 저희 과정 같은 국비 과정 혹시 수강하신 이력 있으실까요? \\n',\n",
       " '아니요 없어요. \\n',\n",
       " '아 처음이세요? 네 그러면 혹시 지금 거주하시는 지역은 어느 쪽이세요? \\n',\n",
       " '어 그 일산 쪽에 거주하고 있어요. \\n',\n",
       " '일산 쪽이세요 혹시 뭐 구나 동까지 말씀 주실 수 있으실까요? 저희가 통화 때문에 확인하는 부분이라서요. \\n',\n",
       " '아 행신력 바로 근처에 거주하고 있음. \\n',\n",
       " '인식력 근처로 잘 남겨드리고요. 네 어 저희가 그 지원하셨을 때 확인은 하셨겠지만 그 개강하고 2주 정도는 온라인 과정이 있고요 이후에 5개월 정도는 성수에 있는 교육장에서 지금 저 좀 교육이 진행이 되거든요. 통학은 문제 없으실까요? \\n',\n",
       " '네 문제 없어요. \\n',\n",
       " '네 그러면 저희가 마지막으로 그 면접 안내드릴 건데요 면접은 저희가 줌을 통해서 비대면으로 진행하고 있어요. 네 주문 설치되어 있으실까요? 네 그러면 혹시 월요일 오후에 면접 가능한 시간 있으실까요 어. \\n',\n",
       " '어 다음 주 월요일. \\n',\n",
       " '네네. \\n',\n",
       " '네 아무 때나 괜찮아요. \\n',\n",
       " '괜찮으세요? 그러면은 면접 시간 2시로 잡아드려도 괜찮으실까요? 네. \\n',\n",
       " '그 비대면 면접이라는 게. \\n',\n",
       " '네 응? \\n',\n",
       " '그냥 줌을 통해서 면접한다는 거죠? \\n',\n",
       " '네네 맞습니다. \\n',\n",
       " '아 네 알겠습니다.\\n',\n",
       " '한 10분에서 15분 정도 진행될 거고요. 저희 과정 신청하신 동기라던가 뭐 아니면은 뭐 커리큘럼에 대한 이해도 그다음에 저희 과정 수료하시고 계획 같은 거 위주로 여쭤볼 거예요. 그래서 네 편하게 참석해 주시면 되고요 면접 참여하시는 URL은 월요일날 오전 중으로 발송될 거예요. \\n',\n",
       " '2시에 네 해당 URL 접속해서 면접 진행해 주시면 됩니다. \\n',\n",
       " '아 알겠습니다. \\n',\n",
       " '네 감사합니다. 네. \\n']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('C:/Users/USER/Desktop/ㄹㄹ.txt', 'r') as f:\n",
    "    tt= f.readlines()\n",
    "tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 텍스트: 딥러닝 부트 캠프를 신청해 주셔서 감사하다고 하고 면접 시간은 2시로 잡아드리기로 하고 면접에 대한 대화를 나눈다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart import BartForConditionalGeneration \n",
    "\n",
    "# 모델 바이너리 파일 경로\n",
    "#model_binary_path = '/content/drive/MyDrive/KoBART-summarization-main/KoBART-summarization-main/kobart_summary'\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "model = BartForConditionalGeneration.from_pretrained('kobart_summary2')\n",
    "summary_texts=[]\n",
    "#input_text =  test_df['documents'][0]\n",
    "# 입력 텍스트를 토큰화하여 인코딩\n",
    "\n",
    "input_text= ' '.join(tt)\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 모델을 사용하여 요약 생성\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=1.0, num_beams=4)\n",
    "\n",
    "# 요약 결과 디코딩\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "summary_texts.append(summary_text)\n",
    "# 요약 출력\n",
    "print(\"요약 텍스트:\", summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('교육장', 0.5726),\n",
       " ('커리큘럼', 0.5471),\n",
       " ('지원서', 0.5364),\n",
       " ('수강', 0.5353),\n",
       " ('캠프', 0.5041)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from keybert import KeyBERT\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "\n",
    "# KoBERT 모델과 토크나이저 불러오기\n",
    "mecab= Mecab('C:/mecab/mecab-ko-dic')\n",
    "text= '여보세요.  네 안녕하세요 최용우님 맞으시죠 네 여기는 딥러닝 부트 캠프 신청해주신 알파코인데요 네 잠시 통화 가능하세요 네 저희 과정 지원해주셔서 감사드리고요. 저희 지원서 바탕으로 몇 가지 여쭤보고 면접 예약 도와드릴게요 네 어 내일 배움 카드는 있다고 체크해 주셨는데 네 네 실물 카드로 갖고 계신 거죠? 네 혹시 저희 과정 같은 국비 과정 혹시 수강하신 이력 있으실까요?  아니요 없어요.  아 처음이세요? 네 그러면 혹시 지금 거주하시는 지역은 어느 쪽이세요?  어 그 일산 쪽에 거주하고 있어요.  일산 쪽이세요 혹시 뭐 구나 동까지 말씀 주실 수 있으실까요? 저희가 통화 때문에 확인하는 부분이라서요.  아 행신력 바로 근처에 거주하고 있음.  인식력 근처로 잘 남겨드리고요. 네 어 저희가 그 지원하셨을 때 확인은 하셨겠지만 그 개강하고 2주 정도는 온라인 과정이 있고요 이후에 5개월 정도는 성수에 있는 교육장에서 지금 저 좀 교육이 진행이 되거든요. 통학은 문제 없으실까요?  네 문제 없어요.  네 그러면 저희가 마지막으로 그 면접 안내드릴 건데요 면접은 저희가 줌을 통해서 비대면으로 진행하고 있어요. 네 주문 설치되어 있으실까요? 네 그러면 혹시 월요일 오후에 면접 가능한 시간 있으실까요 어.  어 다음 주 월요일.  네네.  네 아무 때나 괜찮아요.  괜찮으세요? 그러면은 면접 시간 2시로 잡아드려도 괜찮으실까요? 네.  그 비대면 면접이라는 게.  네 응?  그냥 줌을 통해서 면접한다는 거죠?  네네 맞습니다.  아 네 알겠습니다. 한 10분에서 15분 정도 진행될 거고요. 저희 과정 신청하신 동기라던가 뭐 아니면은 뭐 커리큘럼에 대한 이해도 그다음에 저희 과정 수료하시고 계획 같은 거 위주로 여쭤볼 거예요. 그래서 네 편하게 참석해 주시면 되고요 면접 참여하시는 URL은 월요일날 오전 중으로 발송될 거예요.  2시에 네 해당 URL 접속해서 면접 진행해 주시면 됩니다.  아 알겠습니다.  네 감사합니다. 네.'\n",
    " \n",
    "text= ' '.join(mecab.nouns(text))\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=5)\n",
    "keywords\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "key_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
