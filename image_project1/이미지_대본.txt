안녕하세요, 팀 이게모야의 000 입니다.
‘이’미지 분류 ‘모’델 에서 ‘이모’와 감정을 뜻하는 ‘이모’션에서 각각 ‘이모’를 따왔고 이미지 분류 모델로 감정분류를 어떻게 하고 어떻게 활용할지 생각하여 ‘이’게’모’야? 입니다.
프로젝트 명 ‘표정 피자’는 밝은 표정을 갖자는 의미입니다.

<프로젝트 개요>
먼저, 프로젝트 개요에서는 우리 프로젝트의 아이디어를 소개하겠습니다. 
이 부분에서는 프로젝트의 목적, 배경, 주요 내용 등을 간단히 설명하여 전체적인 맥락을 이해할 수 있도록 하겠습니다.
<프로젝트 구성과 역할>
다음으로, 프로젝트 구성과 역할 부분에서는 우리 프로젝트의 전반적인 절차를 큰 카테고리로 소개하겠습니다.
이 부분에서는 프로젝트를 수행하기 위한 구성, 각 팀원의 역할, 전체적인 진행 과정 등을 설명하여 프로젝트의 전체적인 구조와 운영 방식을 이해할 수 있도록 하겠습니다.
<프로젝트 구성과 방법>
이어서, 프로젝트 구성마다 어떻게 진행했는지 상세하게 소개하겠습니다. 
이 부분에서는 각 단계별로 어떤 활동을 수행했는지, 어떤 결과를 얻었는지 등을 구체적으로 설명하여 청중들이 프로젝트의 진행 과정을 자세히 이해할 수 있도록 하겠습니다.
<프로젝트 결과>
마지막으로, 프로젝트 결과를 소개하겠습니다. 
이 부분에서는 프로젝트를 통해 얻은 성과와 향후 계획 등을 설명하여 프로젝트의 성과와 의의를 이해할 수 있도록 하겠습니다.
이와 같은 순서로 프로젝트를 쉽게 이해할 수 있도록 구조화된 발표를 진행하도록 하겠습니다.



프로젝트 개요입니다.
이미지 분류는 입력된 이미지를 미리 정해진 카테고리 중 하나로 분류하는 컴퓨터 비전 분야의 기본적인 기능입니다. 
이미지 분류는 기계에게 '눈'이 생기게 되는 것이라고 볼 수 있습니다.
눈이 있는 경우와 없는 경우의 차이는 상황을 직접 보고 판단할 수 있느냐 없느냐의 차이가 있다고 할 수 있습니다. 
예를 들어보겠습니다. 회사에서 사람을 채용할 때 전화나 서류만으로 평가하지 않고 직접 면접을 통해 평가하는 이유가 무엇일까요? 
'직접 보고 판단'하는 것이 중요하기 때문입니다.
하지만 사람이 직접 평가하는 것은 공정성과 객관성 측면에서 한계가 있습니다. 
그치만 기계가 이미지 분류 기술을 활용하면 인적 자원의 제약 없이 동일한 기준으로 공정한 평가가 가능할 것으로 기대할 수 있겠습니다.
우리 팀은 이러한 이미지 분류 기술을 활용하여 면접 평가 프로세스를 개선하는 프로젝트를 진행하고자 합니다. 
구체적으로 어떤 방식으로 진행할지는 다음 단계에서 설명드리겠습니다.



이 프로젝트는 면접 평가 과정에서 활용할 수 있는 이미지 분류 모델을 개발하는 것을 목표로 합니다. 
면접 평가 시 필요한 주요 평가 지표를 파악하고, 그중 표정 분석 부분을 이미지 분류 모델로 해결하고자 했습니다.
표정 분석을 통해 평가할 수 있는 부분은 인성 및 태도 영역입니다. 
지원자의 표정, 눈빛 등을 분석하면 성실성, 자신감, 적극성 등을 파악할 수 있습니다.
제일 먼저 면접에 필요한 데이터셋은 무엇일지 탐색했습니다.
그 다음으론 이미지 분류 모델 중 어떤 모델로 분류할지 선정했고
선정한 모델을 학습하고 테스트해봤습니다.
그리고 어떻게하면 분류 성능을 높일지 전략을 짜고 실행하고 평가해봤습니다.
마지막으로 개발한 이미지 분류 모델을 활용하여 면접 평가 서비스를 구현을 시도했습니다. 


프로젝트 관리는 github를 활용했습니다.
상세 항목별 역할 분담은 아래 링크를 확인해 주시기 바랍니다.

다음은 데이터셋 탐색입니다.
AI Hub의 한국인 감정인식을 위한 복합 영상을 활용했습니다.
7개의 감정 데이터셋에서 면접 평가 지표로 꼭 필요하다 판단한 3개 감정으로 추렸습니다.
제한된 프로젝트 리소스로 인해 각 감정별 3천개의 이미지만을 다운받아 사용했습니다.
해당 데이터셋은 각 이미지마다 라벨 정보로 나이 성별 직업 촬영배경 등 json 형태로 맵핑되어 있습니다.
한국인에 최적화 된 서비스를 생각했는데 더 나아가 외국인 데이터셋도 모델 성능에 영향을 줄까? 생각하여 찾아봤습니다.
LFW 이모션 데이터셋 이라는 외국인 감정별 이미지 데이터셋을 준비하긴 했으나 사용해볼 시간이 부족해 아쉽게도 사용해보진 못하고 링크만 아래 기제했습니다.
이 외에도 FER2013라는  7가지 기본 감정 표현이 포함된 이미지 데이터셋과 EmotioNet: 100여 가지 감정 표현이 포함된 대규모 이미지 데이터셋도 있다는 것을 확인했지만 프로젝트 여건상 마찬가지로 있다는 정도만 알고 넘어갔습니다. 

감정별 이미지 데이터 예시 입니다.

저희가 선정한 이미지 분류 모델은 vision transformer와 mlp-mixer라는 모델입니다. 
이 두 모델을 선정하게 된 이유는 우선 vision transformer 기반 모델은 현재도 이미지 classification 분야에서 sota를 차지하고 있고 분류외에도 다양한 컴퓨터 비전 분야에서 활용되기 때문에 연구할 필요가 있다고 생각했습니다.
mlp-mixer 모델 같은 경우에는 vision transformer와 비슷한 시기에 출현했는데 mlp만을 이용해서 vision transformer 정도의 성능을 보여주었기 때문에 효율성 면에서 이점이 있을 것이고 다른 발전 방향이 있을 것이라고 생각하여 선정하였습니다. 

이미지 분류 모델 선정 -1
9. 다음은 선정한 모델에 대해서 설명을 드리겠습니다.
우선 vision transformer 모델입니다. 이 모델은 이미지 처리에 transformer 구조를 적용한 모델로 모델 작동 과정은 다음과 같습니다. 우선 이미지를 P x P 크기의 패치 N개로 자른 후에, 패치별로 flatten하여 N개의 P x P x Channel 차원 벡터로 만듭니다. 이후에 각 패치를 Linear projection을 통해 D차원의 벡터로 변환하여 N개의 D 차원 벡터를 생성합니다. 그리고 여기에 D 차원의 class embedding vector를 추가하여서 총 N+1개의 D차원 벡터를 만들고 마지막으로 N+1개의 position embedding vector를 생성하여 각 패치 임베딩 벡터에 해당하는 position embedding vector를 더해줍니다. 이렇게 하여 최종적으로 임베딩된 패치들은 transformer encoder의 입력으로 들어가서 layer normalization, multi head self attetion, mlp 블록등을 통과하여 output으로 출력됩니다. 이렇게 출력된 output에서 1 x D 차원의 class token 부분을 분류기에 입력하여 클래스 예측을 수행합니다. 저희는 imagenet-21k로 pretrained 된 weight를 fine tuning 하는데 사용하였습니다.


이미지 분류 모델 선정 -2
10. 다음은 mlp-mixer 모델입니다. 이 모델은 구조에 multi layer perceptron만을 이용한 모델입니다. 모델 작동 과정은 다음과 같습니다. 우선 이 모델도 똑같이 이미지를 P x P 크기의 패치 N개로 자르고 각 이미지 패치를 flatten하고 linear projection을 통해서 D차원으로 변환하여 N x D 크기의 벡터를 생성합니다. 이 벡터는 두 개의 mixer layer 통과하게 되는데 각 layer는 두 개의 mlp block을 포함하고 있습니다. 이 N x D 크기의 벡터는 transpose되어서 token mixing layer를 통과하는데 이 과정에서 벡터의 크기는 변하지 않고 모델은 패치들 간 관계를 학습합니다. 그리고 이 벡터는 다시 transpose 되어서 channel mixing layer를 통과하는데 이 과정에서도 벡터의 크기는 변하지 않고 모델은 모델은 각 패치 내의 채널 간의 관계를 학습합니다. 그 후에는 모든 토큰에 대해서 average pooling을 적용하여 N x D 차원의 벡터를 1 x D 차원으로 축소하고 이 벡터를 분류기의 입력으로 사용하여 클래스 예측을 진행합니다. 이 모델 또한 동일한 이미지 셋으로 pre trained된 weight를 사용하여 fine tuning 하였습니다. 


이미지 분류 모델 학습 및 테스트
11. 각 모델의 학습 결과는 다음과 같습니다. 
여기서 x 축은 하나의 step y축은 accuracy와 loss인데 vision transformer가 mlp mixer에 비해 분류 성능이 조금 더 좋은 것을 볼 수 있었습니다.


이미지 분류 모델 성능 향상 전략
12. 저희는 모델을 단순히 fine tuning 하는 것에서 그치지 않고 모델의 분류 성능을 더욱 향상시킬 수 있는 전략을 생각 해보았습니다. 세가지 전략은 모델 앙상블, iterative fine tuning, 특정 부분을 강조하는 이미지 전처리 이후의 fine tuning이고 각 방법을 설명드리겠습니다. 

이미지 분류 모델 성능 향상 전략 -1
13. 우선 앙상블 전략입니다. 저희는 vision transformer와 mlp mixer의 평균 앙상블 모델을 사용하였습니다. 평균 앙상블 모델은 여러 독립 모델을 학습 시킨 후 그 예측 결과의 평균을 취해서 최종 예측을 결정하는 방법이고, 이 방법은 각 모델이 가지고 있는 예측 오류를 다른 모델로 상쇄시켜 모델의 안정성과 성능을 향상 시킬 수 있습니다.

이미지 분류 모델 성능 향상 전략 -2
14. 다음은 iterative fine tuning 전략입니다. 저희는 ai 면접 시에 면접자의 감정을 분석하는 모델을 구현하는 것을 기획하였는데 면접의 주 연령층은 20~30대이기 때문에 해당 target에 대해서 더 분류를 잘하는 모델을 구현할 필요가 있었고, 따라서 전체 연령층에 대해서 fine tuning된 모델을 다시 20~30대 연령층의 데이터로 fine tuning하는 전략을 세웠습니다. 

이미지 분류 모델 성능 향상 전략 -3
15. 마지막으로 이미지 전처리 후 fine tuning 하는 전략입니다. 학습 데이터 셋은 다양한 장소에서 촬영된 이미지로 배경에 사람 형체의 객체가 간혹 혼재하기 때문에 인물에 집중되도록 하는 방법을 사용하고자 했고 물론 object detection을 통해 인물의 얼굴을 잡으면 성능이 가장 향상 되겠지만 object detection을 사용하지 않는 방법을 고려하였습니다. 따라서 피부 톤을 색상, 채도, 명도로 구분하여 일반적인 사람의 피부색을 검출하는 이미지 전처리 방식을 사용하여 fine tuning을 진행하였습니다.  

모델 성능 향상 평가
16. 모델의 성능 결과는 다음과 같습니다.
test data set은 20~30대 연령층의 얼굴 이미지 데이터 입니다.
앙상블 모델은 기존 vit보다 성능이 좋아졌지만 다른 fine tuning 전략은 과적합으로 인해 오히려 성능이 낮아진 결과를 얻었습니다. 


활용 분야
얼굴 감정 분류로 면접외에 어떻게 더 활용이 가능할까요?
실현 가능성은 낮을수 있지만 몇가지 추려봤습니다.
제일 먼저 앨범에 최근 저장된 셀카 사진을 위주로 음악, 영화, 쇼핑을 추천해줄 수 있을수도 있겠고
표정을 분석해 고객 니즈를 파악해 알맞은 전략을 짜고 상품 제작에 힘쓸 수도 있고
시각장애인은 음성을 통해서만 상대의 감정을 파악해야하는데 표정분석 서비스로 상대의 감정을 신호로 줄 수 있게하면 좋을것이라 생각해볼 수 있습니다.

다음은 서비스 예시 입니다.
지원자의 표정 이미지를 입력받아 자동으로 분석하고, 인성 및 태도 평가 결과를 제공하는 형태입니다.
이를 통해 면접관의 주관적 평가를 최소화하고, 보다 공정하고 객관적인 면접 평가가 가능할 것으로 기대하며 작업했습니다.
스트림릿으로 간단한 웹 서비스 구현 해보려했는데 실제 현업에 더 잘 쓰인다는 FastAPI와 Uvicorn을 활용해서 구현하기로 방향을 변경하고 학습하다보니 시간 지체가 있었고 생각했던것보다 기간내에 서비스를 만들기에 어려움이  있어 아쉽게도 보여드리지 못하고 넘어가겠습니다.

지금까지 사용한 기술 스택은 화면을 참고해주세요.

자체평가는 다음 화면과 같습니다.

감사합니다.
