{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\torch_mlp_mixer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\USER\\\\Desktop'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "checkpoint = torch.load(\"C:/Users/USER/Downloads/vit_base_p16_224-80ecf9dd.pth\", map_location='cpu') # GPU 환경이 아닌 경우 'cpu'를 사용합니다.\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 모델의 마지막 분류기 부분을 새로운 클래스 수에 맞게 변경\n",
    "num_classes = 3  # 새로운 클래스 수\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류기만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 모든 파라미터를 동결\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 출력층 파라미터만 학습 가능하도록 설정\n",
    "model.head.weight.requires_grad = True\n",
    "model.head.bias.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모폴로지 데이터 전처리\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "class MorphologyTransform:\n",
    "    def __call__(self, img):\n",
    "        # img는 PIL 이미지\n",
    "        img = np.array(img)\n",
    "        kernel = np.ones((5,5), np.uint8)\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "        return Image.fromarray(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir,annotations_dir,  transform=None):\n",
    "        \"\"\"\n",
    "        annotation_dir (string): 메타데이터가 있는 JSON 파일의 경로\n",
    "        img_dir (string): 모든 이미지가 있는 디렉토리의 경로\n",
    "        transform (callable, optional): 샘플에 적용될 선택적 변환\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.annotation_dir= annotations_dir\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        return len(label_list)\n",
    "    \n",
    "    def preprocess(self, image_path):\n",
    "        # 이미지 로드 및 HSV 변환\n",
    "        with open(image_path, 'rb') as f:\n",
    "            arr = np.asarray(bytearray(f.read()), dtype=np.uint8)\n",
    "            image = cv2.imdecode(arr, -1)\n",
    "        \n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # 스킨 색상 범위 설정\n",
    "        lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "        upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "\n",
    "        # 스킨 색상 마스크 생성\n",
    "        skin_mask = cv2.inRange(hsv_image, lower_skin, upper_skin)\n",
    "\n",
    "        # 마스크를 사용하여 스킨 색상 영역 추출\n",
    "        skin = cv2.bitwise_and(image, image, mask=skin_mask)\n",
    "\n",
    "        # OpenCV 이미지를 PIL 이미지로 변환 (transforms와 호환되도록)\n",
    "        skin_pil = Image.fromarray(cv2.cvtColor(skin, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        return skin_pil\n",
    "    \n",
    "    def mopology(self, image_path):\n",
    "        with open(image_path, 'rb') as f:\n",
    "            arr = np.asarray(bytearray(f.read()), dtype=np.uint8)\n",
    "            image = cv2.imdecode(arr, -1)\n",
    "\n",
    "            kernel = np.ones((5,5), np.uint8)\n",
    "            img = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "            return Image.fromarray(img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, label_list[idx].split('.')[0]+'.'+label_list[idx].split('.')[1])\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            preprocessed_image = self.mopology(img_path)\n",
    "        except (IOError, OSError) as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        # preprocessed_image = self.preprocess(img_path)\n",
    "        \n",
    "        \n",
    "        # faceExp_uploader 부분만 라벨로 사용\n",
    "        with open(self.annotation_dir+'/'+label_list[idx],'r', encoding='utf-8') as f:\n",
    "            self.image_labels=json.load(f)\n",
    "        label = self.image_labels['faceExp_uploader']\n",
    "        label_to_int = {'기쁨': 0, '당황': 1, '중립': 2, '불안':3}\n",
    "\n",
    "        # 문자열 라벨을 정수로 매핑\n",
    "        label_int = label_to_int[label]\n",
    "        label_tensor = torch.tensor(label_int, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(preprocessed_image)\n",
    "        else:\n",
    "            # 기본적으로 이미지를 Tensor로 변환\n",
    "            transform = ToTensor()\n",
    "            image_tensor = transform(preprocessed_image)\n",
    "        \n",
    "        return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(img_size, train_img_dir,train_annotation_dir, test_img_dir, test_annotation_dir, train_batch_size, eval_batch_size):\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        #주어진 이미지에서 임의의 크기 및 비율로 크롭한 후, 지정된 크기로 이미지를 리사이즈합니다, 5%~100% 사이의 영역을 crop 하여 resizing, crop되는 부분은 랜덤, ex) 중앙하단, 왼쪽 상단 등\n",
    "        transforms.RandomResizedCrop((img_size, img_size), scale=(0.05, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # 픽셀에 normalize 적용, 각 채널의 평균과 표준 편차, pre trained된 데이터의 통계를 기반으로 설정\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    trainset = CustomDataset(img_dir=train_img_dir,\n",
    "                             annotations_dir=train_annotation_dir,\n",
    "                             transform=transform_train)\n",
    "    testset = CustomDataset(img_dir=test_img_dir,\n",
    "                            annotations_dir=test_annotation_dir,\n",
    "                            transform=transform_test)\n",
    "    \n",
    "\n",
    "    train_sampler = RandomSampler(trainset) # 데이터셋에서 무작위로 샘플을 선택,  데이터셋의 인덱스를 무작위로 섞어서 데이터의 순서를 랜덤하게 배치, 모델이 순서에 의존하지 않고 학습함\n",
    "    test_sampler = SequentialSampler(testset) # 데이터셋에서 순차적으로 샘플을 선택, 데이터를 처음부터 끝까지 순서대로 샘플링\n",
    "    train_loader = DataLoader(trainset,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=train_batch_size,\n",
    "                              num_workers=0,\n",
    "                              pin_memory=True)\n",
    "    test_loader = DataLoader(testset,\n",
    "                             sampler=test_sampler,\n",
    "                             batch_size=eval_batch_size,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True) if testset is not None else None\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 시 하이퍼 파라미터 지정\n",
    "num_epochs=15\n",
    "train_batch_size = 64  # 훈련 배치 크기\n",
    "eval_batch_size = 64  # 평가 배치 크기\n",
    "train_img_dir= 'C:/Users/USER/Desktop/data/train_img'\n",
    "train_annotation_dir= 'C:/Users/USER/Desktop/data/train_label'\n",
    "test_img_dir='C:/Users/USER/Desktop/data/valid_img'\n",
    "test_annotaion_dir='C:/Users/USER/Desktop/data/valid_label'\n",
    "name= 'vit_class_3_final_pre'\n",
    "output_dir= 'output'\n",
    "eval_every = 113  # 몇 스텝마다 평가를 할 것인지\n",
    "learning_rate = 3e-2  # 초기 학습률\n",
    "weight_decay = 0  # 가중치 감소율\n",
    "sweight_decay = 0  # 가중치 감소율\n",
    "num_steps = eval_every * num_epochs  # 총 훈련 스텝\n",
    "seed = 42  # 초기화를 위한 랜덤 시드\n",
    "n_gpu=1\n",
    "gradient_accumulation_steps = 1  # 업데이트를 위해 누적할 스텝 수\n",
    "warmup_steps = 150  # 웜업을 위한 스텝 수\n",
    "max_grad_norm = 1.0  # 최대 그래디언트 노름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|| 0/113 [00:00<?, ?it/s]c:\\Users\\USER\\anaconda3\\envs\\torch_mlp_mixer\\lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "c:\\Users\\USER\\anaconda3\\envs\\torch_mlp_mixer\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epoch 1/15 (Step 1 / 1695) (Loss=1.13424):   0%|| 0/113 [00:08<?, ?it/s]c:\\Users\\USER\\anaconda3\\envs\\torch_mlp_mixer\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:271: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Validating... (loss=1.00674): 100%|| 29/29 [03:36<00:00,  7.47s/it]00:09,  9.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 113\n",
      "Valid Loss: 0.54975\n",
      "Valid Accuracy: 0.77889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 (Step 113 / 1695) (Loss=0.46333): 100%|| 113/113 [20:20<00:00, 10.81s/it]\n",
      "Validating... (loss=0.91786): 100%|| 29/29 [03:37<00:00,  7.50s/it]00:09,  9.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 226\n",
      "Valid Loss: 0.51118\n",
      "Valid Accuracy: 0.80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 (Step 226 / 1695) (Loss=0.50482): 100%|| 113/113 [20:21<00:00, 10.81s/it]\n",
      "Validating... (loss=0.60925): 100%|| 29/29 [03:37<00:00,  7.50s/it]00:09,  9.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 339\n",
      "Valid Loss: 0.38983\n",
      "Valid Accuracy: 0.84778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 (Step 339 / 1695) (Loss=0.53568): 100%|| 113/113 [20:19<00:00, 10.80s/it]\n",
      "Validating... (loss=0.60262): 100%|| 29/29 [03:38<00:00,  7.53s/it]00:08,  8.65s/it]\n",
      "Epoch 4/15 (Step 452 / 1695) (Loss=0.48682): 100%|| 113/113 [20:21<00:00, 10.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 452\n",
      "Valid Loss: 0.43171\n",
      "Valid Accuracy: 0.82389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.59468): 100%|| 29/29 [03:36<00:00,  7.48s/it]00:09,  9.05s/it]\n",
      "Epoch 5/15 (Step 565 / 1695) (Loss=0.61340): 100%|| 113/113 [20:17<00:00, 10.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 565\n",
      "Valid Loss: 0.56080\n",
      "Valid Accuracy: 0.75000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.80358): 100%|| 29/29 [03:37<00:00,  7.50s/it]00:09,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 678\n",
      "Valid Loss: 0.38312\n",
      "Valid Accuracy: 0.86222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 (Step 678 / 1695) (Loss=0.31624): 100%|| 113/113 [20:16<00:00, 10.77s/it]\n",
      "Epoch 7/15 (Step 702 / 1695) (Loss=0.34751):  21%|| 24/113 [03:33<13:14,  8.93s/it]"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#스케줄러\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\" 학습 초기에는 학습률을 점진적으로 증가시키는 warmup기간을 갖고 그 이후에는 코사인 함수를 따라 학습률을 감소시킴\n",
    "        스케줄러는 optimizer에 설정된 학습률을 기준으로 하여, 특정 시점에서의 학습률을 조절하는 비율(factor)을 계산\n",
    "        warmup 기간 내: step / warmup_steps 비율로 학습률을 증가, 초기 학습률 x step/warmup_steps\n",
    "        warmup 기간 이후: 초기 학습률 x 0.5 * (1 + cos(π * cycles * 2 * progress)) 공식에 따라 학습률이 감소\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def valid(model, writer, test_loader,eval_batch_size, global_step,device):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            logits = model(x)\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    print(\"Global Steps: %d\" % global_step)\n",
    "    print(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    print(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    logger.info(\"/n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Global Steps: %d\" % global_step)\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    writer.add_scalar(\"test/loss\", scalar_value=eval_losses.avg, global_step=global_step)\n",
    "    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, output_dir, name):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(output_dir, \"%s_checkpoint.bin\" % name)\n",
    "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
    "    logger.info(\"Saved model checkpoint to [DIR: %s]\", output_dir)\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"logs\", name))\n",
    "\n",
    "train_loader, test_loader = get_loader(224, train_img_dir, train_annotation_dir, test_img_dir, test_annotaion_dir, train_batch_size, eval_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=weight_decay)\n",
    "t_total = num_steps\n",
    "\n",
    "# 스케줄러\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "\n",
    "model.zero_grad()\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "losses = AverageMeter()\n",
    "global_step, best_acc = 0, 0\n",
    "\n",
    "# epoch 단위로 학습 시작\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_iterator = tqdm(train_loader,\n",
    "                          desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        outputs = model(x)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        #loss 계산\n",
    "        loss = loss_fn(outputs, y)\n",
    "        #역전파 학습, 기울기 계산\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0: # 여러 스텝에 걸쳐 gradient를 축적하고 모델에 업데이트, gpu 메모리가 작을 때 큰 배치사이즈를 사용 가능\n",
    "            #loss update\n",
    "            losses.update(loss.item() * gradient_accumulation_steps)\n",
    "            #임계값이 넘는 gradient를 임계값으로 수정, gradient 폭발 방지\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            scheduler.step() # 새로운 학습률을 계산하고 optimizer에 지정되어 있는 학습률 update\n",
    "            optimizer.step() # weight update\n",
    "            optimizer.zero_grad() # 이전 gradient 초기화\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_iterator.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} (Step {global_step} / {t_total}) (Loss={losses.val:.5f})\"\n",
    "            )\n",
    "            writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
    "            writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                accuracy = valid(model, writer, test_loader, eval_batch_size, global_step, device)\n",
    "\n",
    "                if best_acc < accuracy:\n",
    "                    save_model(model, output_dir, name)\n",
    "                    best_acc = accuracy\n",
    "                model.train()\n",
    "    \n",
    "    losses.reset() # 각 epoch의 끝에 loss 초기화\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류기만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|| 0/113 [00:00<?, ?it/s]c:\\Users\\USER\\anaconda3\\envs\\torch_mlp_mixer\\lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "c:\\Users\\USER\\anaconda3\\envs\\torch_mlp_mixer\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epoch 1/15 (Step 1 / 1695) (Loss=1.23820):   0%|| 0/113 [00:06<?, ?it/s]c:\\Users\\USER\\anaconda3\\envs\\torch_mlp_mixer\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:271: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Validating... (loss=0.44943): 100%|| 29/29 [02:53<00:00,  6.00s/it]00:06,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 113\n",
      "Valid Loss: 0.76957\n",
      "Valid Accuracy: 0.63778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 (Step 113 / 1695) (Loss=0.80464): 100%|| 113/113 [15:16<00:00,  8.11s/it]\n",
      "Validating... (loss=0.78176): 100%|| 29/29 [02:53<00:00,  5.97s/it]00:06,  6.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 226\n",
      "Valid Loss: 0.73327\n",
      "Valid Accuracy: 0.67167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 (Step 226 / 1695) (Loss=0.66969): 100%|| 113/113 [15:19<00:00,  8.14s/it]\n",
      "Validating... (loss=1.04426): 100%|| 29/29 [02:54<00:00,  6.02s/it]00:06,  6.77s/it]\n",
      "Epoch 3/15 (Step 339 / 1695) (Loss=0.88313): 100%|| 113/113 [15:18<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 339\n",
      "Valid Loss: 0.76911\n",
      "Valid Accuracy: 0.64833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.59317): 100%|| 29/29 [02:52<00:00,  5.96s/it]00:06,  6.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 452\n",
      "Valid Loss: 0.69109\n",
      "Valid Accuracy: 0.69000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 (Step 452 / 1695) (Loss=0.74933): 100%|| 113/113 [15:19<00:00,  8.14s/it]\n",
      "Validating... (loss=1.10554): 100%|| 29/29 [02:48<00:00,  5.82s/it]00:06,  6.66s/it]\n",
      "Epoch 5/15 (Step 565 / 1695) (Loss=0.99390): 100%|| 113/113 [15:12<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 565\n",
      "Valid Loss: 0.74479\n",
      "Valid Accuracy: 0.67111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.46549): 100%|| 29/29 [02:53<00:00,  5.99s/it]00:06,  6.77s/it]\n",
      "Epoch 6/15 (Step 678 / 1695) (Loss=0.65102): 100%|| 113/113 [15:10<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 678\n",
      "Valid Loss: 0.68713\n",
      "Valid Accuracy: 0.68611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.59288): 100%|| 29/29 [02:52<00:00,  5.95s/it]00:06,  6.38s/it]\n",
      "Epoch 7/15 (Step 791 / 1695) (Loss=0.48212): 100%|| 113/113 [15:15<00:00,  8.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 791\n",
      "Valid Loss: 0.69982\n",
      "Valid Accuracy: 0.67667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.68618): 100%|| 29/29 [02:53<00:00,  5.98s/it]00:06,  6.49s/it]\n",
      "Epoch 8/15 (Step 904 / 1695) (Loss=0.76913): 100%|| 113/113 [15:20<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 904\n",
      "Valid Loss: 0.72334\n",
      "Valid Accuracy: 0.66556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.67977): 100%|| 29/29 [02:52<00:00,  5.95s/it]<00:06,  6.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1017\n",
      "Valid Loss: 0.65842\n",
      "Valid Accuracy: 0.69389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 (Step 1017 / 1695) (Loss=0.84269): 100%|| 113/113 [15:17<00:00,  8.12s/it]\n",
      "Validating... (loss=0.63227): 100%|| 29/29 [02:51<00:00,  5.93s/it]6<00:06,  6.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1130\n",
      "Valid Loss: 0.63933\n",
      "Valid Accuracy: 0.72167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 (Step 1130 / 1695) (Loss=0.58100): 100%|| 113/113 [15:18<00:00,  8.13s/it]\n",
      "Validating... (loss=0.55063): 100%|| 29/29 [02:52<00:00,  5.96s/it]8<00:06,  6.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1243\n",
      "Valid Loss: 0.63465\n",
      "Valid Accuracy: 0.73111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 (Step 1243 / 1695) (Loss=0.86004): 100%|| 113/113 [15:21<00:00,  8.15s/it]\n",
      "Validating... (loss=0.67289): 100%|| 29/29 [02:50<00:00,  5.90s/it]6<00:06,  6.52s/it]\n",
      "Epoch 12/15 (Step 1356 / 1695) (Loss=0.47857): 100%|| 113/113 [15:17<00:00,  8.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1356\n",
      "Valid Loss: 0.62253\n",
      "Valid Accuracy: 0.72833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.75699): 100%|| 29/29 [02:50<00:00,  5.89s/it]3<00:06,  6.59s/it]\n",
      "Epoch 13/15 (Step 1469 / 1695) (Loss=0.59841): 100%|| 113/113 [15:04<00:00,  8.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1469\n",
      "Valid Loss: 0.63534\n",
      "Valid Accuracy: 0.71278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.70626): 100%|| 29/29 [02:49<00:00,  5.86s/it]0<00:06,  6.39s/it]\n",
      "Epoch 14/15 (Step 1582 / 1695) (Loss=0.54780): 100%|| 113/113 [15:00<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1582\n",
      "Valid Loss: 0.63188\n",
      "Valid Accuracy: 0.71889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=0.68540): 100%|| 29/29 [02:50<00:00,  5.88s/it]0<00:06,  6.55s/it]\n",
      "Epoch 15/15 (Step 1695 / 1695) (Loss=0.69411): 100%|| 113/113 [15:00<00:00,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1695\n",
      "Valid Loss: 0.62623\n",
      "Valid Accuracy: 0.72222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#스케줄러\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\" 학습 초기에는 학습률을 점진적으로 증가시키는 warmup기간을 갖고 그 이후에는 코사인 함수를 따라 학습률을 감소시킴\n",
    "        스케줄러는 optimizer에 설정된 학습률을 기준으로 하여, 특정 시점에서의 학습률을 조절하는 비율(factor)을 계산\n",
    "        warmup 기간 내: step / warmup_steps 비율로 학습률을 증가, 초기 학습률 x step/warmup_steps\n",
    "        warmup 기간 이후: 초기 학습률 x 0.5 * (1 + cos(π * cycles * 2 * progress)) 공식에 따라 학습률이 감소\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def valid(model, writer, test_loader,eval_batch_size, global_step,device):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            logits = model(x)\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    print(\"Global Steps: %d\" % global_step)\n",
    "    print(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    print(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    logger.info(\"/n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Global Steps: %d\" % global_step)\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, output_dir, name):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(output_dir, \"%s_checkpoint.bin\" % name)\n",
    "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
    "    logger.info(\"Saved model checkpoint to [DIR: %s]\", output_dir)\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"logs\", name))\n",
    "\n",
    "train_loader, test_loader = get_loader(224, train_img_dir, train_annotation_dir, test_img_dir, test_annotaion_dir, train_batch_size, eval_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                            lr=learning_rate,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=weight_decay)\n",
    "t_total = num_steps\n",
    "\n",
    "# 스케줄러\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "\n",
    "model.zero_grad()\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "losses = AverageMeter()\n",
    "global_step, best_acc = 0, 0\n",
    "\n",
    "# epoch 단위로 학습 시작\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_iterator = tqdm(train_loader,\n",
    "                          desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        outputs = model(x)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        #loss 계산\n",
    "        loss = loss_fn(outputs, y)\n",
    "        #역전파 학습, 기울기 계산\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0: # 여러 스텝에 걸쳐 gradient를 축적하고 모델에 업데이트, gpu 메모리가 작을 때 큰 배치사이즈를 사용 가능\n",
    "            #loss update\n",
    "            losses.update(loss.item() * gradient_accumulation_steps)\n",
    "            #임계값이 넘는 gradient를 임계값으로 수정, gradient 폭발 방지\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            scheduler.step() # 새로운 학습률을 계산하고 optimizer에 지정되어 있는 학습률 update\n",
    "            optimizer.step() # weight update\n",
    "            optimizer.zero_grad() # 이전 gradient 초기화\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_iterator.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} (Step {global_step} / {t_total}) (Loss={losses.val:.5f})\"\n",
    "            )\n",
    "            writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
    "            writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                accuracy = valid(model, writer, test_loader, eval_batch_size, global_step, device)\n",
    "\n",
    "                if best_acc < accuracy:\n",
    "                    save_model(model, output_dir, name)\n",
    "                    best_acc = accuracy\n",
    "                model.train()\n",
    "    \n",
    "    losses.reset() # 각 epoch의 끝에 loss 초기화\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
