{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlp mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (stem): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "mixer = timm.create_model('mixer_b16_224', pretrained=True)\n",
    "num_classes = 3  # 새로운 클래스 수\n",
    "mixer.head = nn.Linear(mixer.head.in_features, num_classes)\n",
    "mixer.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "checkpoint = torch.load(\"C:/Users/USER/Desktop/output/mlp_mixer_class_3_final_final_checkpoint.bin\", map_location='cpu') # GPU 환경이 아닌 경우 'cpu'를 사용합니다.\n",
    "mixer.load_state_dict(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "\n",
    "num_classes = 3  # 새로운 클래스 수\n",
    "vit.head = nn.Linear(vit.head.in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "checkpoint = torch.load(\"C:/Users/USER/Desktop/output/vit_class_3_final_final_89.556.bin\", map_location='cpu') # GPU 환경이 아닌 경우 'cpu'를 사용합니다.\n",
    "vit.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 함수 정의\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),  # 모델에 맞는 크기로 조정\n",
    "    transforms.CenterCrop(224),  # 중앙을 기준으로 자르기\n",
    "    transforms.ToTensor(),  # Tensor로 변환\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 정규화\n",
    "])\n",
    "\n",
    "# 이미지 불러오기 및 전처리\n",
    "image_path = \"C:/Users/USER/Downloads/이상한표정.jpg\"\n",
    "input_image = Image.open(image_path)\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # 배치 차원을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-Mixer (%): [[81.28 17.58  1.14]]\n",
      "VIT (%) : [[18.27 81.27  0.46]]\n",
      "평균 앙상블 모델 (%): [[49.77 49.42  0.8 ]]\n",
      "Final prediction: 기쁨\n",
      "평균 앙상블 모델 (%): [49.77 49.42  0.8 ]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 출력 형식을 소수점 두 번째 자리로 설정\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "expression_dict= {0:'기쁨',1:'당황', 2:'중립'}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # MLP-Mixer 예측\n",
    "    mlp_output = mixer(input_batch)\n",
    "    mlp_probs = F.softmax(mlp_output, dim=1)\n",
    "    \n",
    "    # ViT 예측\n",
    "    vit_output = vit(input_batch)\n",
    "    vit_probs = F.softmax(vit_output, dim=1)\n",
    "    \n",
    "    # 평균 앙상블\n",
    "    avg_probs = (mlp_probs + vit_probs) / 2\n",
    "    final_prediction = avg_probs.argmax(dim=1).item()\n",
    "\n",
    "# 각각의 모델 예측 확률 및 최종 예측 확률 출력\n",
    "print(\"MLP-Mixer (%):\", (mlp_probs * 100).numpy().round(2))\n",
    "print(\"VIT (%) :\", (vit_probs * 100).numpy().round(2))\n",
    "print(\"평균 앙상블 모델 (%):\", (avg_probs * 100).numpy().round(2))\n",
    "str_ex= expression_dict[final_prediction]\n",
    "print(\"Final prediction:\", str_ex)\n",
    "\n",
    "# 각 클래스별 평균 확률 출력\n",
    "avg_probs_per_class = (avg_probs.mean(dim=0) * 100).numpy().round(2)\n",
    "print(\"평균 앙상블 모델 (%):\", avg_probs_per_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 데이터 셋으로 f1 score 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir,annotations_dir,  transform=None):\n",
    "        \"\"\"\n",
    "        annotation_dir (string): 메타데이터가 있는 JSON 파일의 경로\n",
    "        img_dir (string): 모든 이미지가 있는 디렉토리의 경로\n",
    "        transform (callable, optional): 샘플에 적용될 선택적 변환\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.annotation_dir= annotations_dir\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        return len(label_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label_list= os.listdir(self.annotation_dir)\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, label_list[idx].split('.')[0]+'.'+label_list[idx].split('.')[1])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except (IOError, OSError) as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # faceExp_uploader 부분만 라벨로 사용\n",
    "        with open(self.annotation_dir+'/'+label_list[idx],'r', encoding='utf-8') as f:\n",
    "            self.image_labels=json.load(f)\n",
    "        label = self.image_labels['faceExp_uploader']\n",
    "        label_to_int = {'기쁨': 0, '당황': 1, '중립': 2}\n",
    "\n",
    "        # 문자열 라벨을 정수로 매핑\n",
    "        label_int = label_to_int[label]\n",
    "        label_tensor = torch.tensor(label_int, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        else:\n",
    "            # 기본적으로 이미지를 Tensor로 변환\n",
    "            transform = ToTensor()\n",
    "            image_tensor = transform(image)\n",
    "        \n",
    "        return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(img_size,  test_img_dir, test_annotation_dir, eval_batch_size):\n",
    "\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    \n",
    "    testset = CustomDataset(img_dir=test_img_dir,\n",
    "                            annotations_dir=test_annotation_dir,\n",
    "                            transform=transform_test)\n",
    "    \n",
    "\n",
    "    test_sampler = RandomSampler(testset) \n",
    "    \n",
    "    test_loader = DataLoader(testset,\n",
    "                             sampler=test_sampler,\n",
    "                             batch_size=eval_batch_size,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True) if testset is not None else None\n",
    "\n",
    "    return  test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 및 데이터로더 설정\n",
    "test_img_dir = \"C:/Users/USER/Desktop/test_img_2030\"\n",
    "test_annotation_dir = \"C:/Users/USER/Desktop/test_label_2030\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=CustomDataset(img_dir=test_img_dir,\n",
    "                            annotations_dir=test_annotation_dir,\n",
    "                            transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 57/57 [03:08<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9100\n",
      "Recall: 0.9100\n",
      "F1 Score: 0.9099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "mixer.to(device)\n",
    "vit.to(device)\n",
    "\n",
    "eval_batch_size = 32\n",
    "test_loader = get_loader(224, test_img_dir, test_annotation_dir, eval_batch_size)\n",
    "\n",
    "# 평가 모드로 전환\n",
    "mixer.eval()\n",
    "vit.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_batch, labels in tqdm(test_loader, desc=\"Evaluating\"):  # tqdm을 사용하여 진행 상태 표시\n",
    "        input_batch, labels = input_batch.to(device), labels.to(device)  # 데이터를 GPU로 이동\n",
    "        \n",
    "        # 모델 예측\n",
    "        mlp_output = mixer(input_batch)\n",
    "        mlp_probs = F.softmax(mlp_output, dim=1)\n",
    "        \n",
    "        vit_output = vit(input_batch)\n",
    "        vit_probs = F.softmax(vit_output, dim=1)\n",
    "        \n",
    "        # 평균 앙상블\n",
    "        avg_probs = (mlp_probs + vit_probs) / 2\n",
    "        predictions = avg_probs.argmax(dim=1)\n",
    "        \n",
    "        # 실제 라벨과 예측 라벨을 저장 (CPU로 이동하여 numpy 변환)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# 정확도, 재현율, F1 스코어 계산\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ViT: 100%|██████████| 57/57 [02:59<00:00,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT - Accuracy: 0.8917, Recall: 0.8917, F1 Score: 0.8912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "mixer.to(device)\n",
    "vit.to(device)\n",
    "\n",
    "eval_batch_size = 32\n",
    "test_loader = get_loader(224, test_img_dir, test_annotation_dir, eval_batch_size)\n",
    "\n",
    "# 평가 모드로 전환\n",
    "mixer.eval()\n",
    "vit.eval()\n",
    "\n",
    "# # MLP Mixer 모델의 메트릭 계산\n",
    "# all_labels_mixer = []\n",
    "# all_predictions_mixer = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for input_batch, labels in tqdm(test_loader, desc=\"Evaluating MLP Mixer\"):\n",
    "#         input_batch, labels = input_batch.to(device), labels.to(device)\n",
    "        \n",
    "#         # 모델 예측\n",
    "#         mlp_output = mixer(input_batch)\n",
    "#         mlp_probs = F.softmax(mlp_output, dim=1)\n",
    "#         predictions = mlp_probs.argmax(dim=1)\n",
    "        \n",
    "#         # 실제 라벨과 예측 라벨을 저장 (CPU로 이동하여 numpy 변환)\n",
    "#         all_labels_mixer.extend(labels.cpu().numpy())\n",
    "#         all_predictions_mixer.extend(predictions.cpu().numpy())\n",
    "\n",
    "# # 메트릭 계산\n",
    "# accuracy_mixer = accuracy_score(all_labels_mixer, all_predictions_mixer)\n",
    "# recall_mixer = recall_score(all_labels_mixer, all_predictions_mixer, average='macro')\n",
    "# f1_mixer = f1_score(all_labels_mixer, all_predictions_mixer, average='macro')\n",
    "# print(f\"MLP Mixer - Accuracy: {accuracy_mixer:.4f}, Recall: {recall_mixer:.4f}, F1 Score: {f1_mixer:.4f}\")\n",
    "\n",
    "# ViT 모델의 메트릭 계산\n",
    "all_labels_vit = []\n",
    "all_predictions_vit = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_batch, labels in tqdm(test_loader, desc=\"Evaluating ViT\"):\n",
    "        input_batch, labels = input_batch.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        vit_output = vit(input_batch)\n",
    "        vit_probs = F.softmax(vit_output, dim=1)\n",
    "        predictions = vit_probs.argmax(dim=1)\n",
    "        \n",
    "        # 실제 라벨과 예측 라벨을 저장 (CPU로 이동하여 numpy 변환)\n",
    "        all_labels_vit.extend(labels.cpu().numpy())\n",
    "        all_predictions_vit.extend(predictions.cpu().numpy())\n",
    "\n",
    "# 메트릭 계산\n",
    "accuracy_vit = accuracy_score(all_labels_vit, all_predictions_vit)\n",
    "recall_vit = recall_score(all_labels_vit, all_predictions_vit, average='weighted')\n",
    "f1_vit = f1_score(all_labels_vit, all_predictions_vit, average='weighted')\n",
    "print(f\"ViT - Accuracy: {accuracy_vit:.4f}, Recall: {recall_vit:.4f}, F1 Score: {f1_vit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8916666666666667\n",
      "F1 Score: 0.8912300921734742\n",
      "Recall: 0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    accuracy = np.mean(all_preds == all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    \n",
    "    return accuracy, f1, recall\n",
    "\n",
    "# 모델과 데이터 로더 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit.to(device)\n",
    "\n",
    "test_loader = get_loader(img_size=224, test_img_dir=test_img_dir, test_annotation_dir=test_annotation_dir, eval_batch_size=32)\n",
    "accuracy, f1, recall = evaluate(vit, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT - Accuracy: 0.8961, Recall: 0.8961, F1 Score: 0.8954 #두번 fine tuning\n",
    "ViT - Accuracy: 0.8917, Recall: 0.8917, F1 Score: 0.8912\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 600, 0: 600, 2: 600})\n",
      "Predictions distribution: {0: 613, 1: 563, 2: 624}\n",
      "Accuracy: 0.8916666666666667\n",
      "F1 Score: 0.8912300921734742\n",
      "Recall: 0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    accuracy = np.mean(all_preds == all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    \n",
    "    return accuracy, f1, recall\n",
    "\n",
    "# 모델과 데이터 로더 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit.to(device)\n",
    "\n",
    "# 데이터셋 및 데이터로더 설정\n",
    "test_loader = get_loader(img_size=224, test_img_dir=test_img_dir, test_annotation_dir=test_annotation_dir, eval_batch_size=32)\n",
    "\n",
    "# 클래스 분포 확인\n",
    "check_class_distribution(test_loader.dataset)\n",
    "\n",
    "# 모델 예측 확인\n",
    "check_model_predictions(vit, test_loader, device)\n",
    "\n",
    "# 모델 평가\n",
    "accuracy, f1, recall = evaluate(vit, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9022222222222223\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          기쁨     0.9345    0.9517    0.9430       600\n",
      "          당황     0.8690    0.8733    0.8712       600\n",
      "          중립     0.9027    0.8817    0.8921       600\n",
      "\n",
      "    accuracy                         0.9022      1800\n",
      "   macro avg     0.9021    0.9022    0.9021      1800\n",
      "weighted avg     0.9021    0.9022    0.9021      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_detailed(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=['기쁨', '당황', '중립'], digits=4)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, class_report\n",
    "\n",
    "# 모델 평가\n",
    "accuracy, class_report = evaluate_detailed(vit, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
